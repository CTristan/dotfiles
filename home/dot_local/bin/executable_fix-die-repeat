#!/usr/bin/env bash
#
# fix-die-repeat - Automated check, review, and fix loop using pi
#
# Usage: fix-die-repeat [OPTIONS]
#
# Options:
#   -c, --check-cmd CMD      Command to run checks (default: ./scripts/ci.sh)
#   -n, --max-iters N        Maximum loop iterations (default: 10)
#       --archive-artifacts  Archive existing artifacts to a timestamped folder
#       --no-compact         Skip automatic compaction of large artifacts
#       --pr-review          Enable PR review mode
#   -h, --help               Show this help message
#
# Environment variables:
#   FDR_CHECK_CMD, FDR_MAX_ITERS, FDR_ARCHIVE_ARTIFACTS, FDR_COMPACT_ARTIFACTS, FDR_PR_REVIEW
#

set -euo pipefail

# Defaults (can be overridden by env vars)
FDR_CHECK_CMD="${FDR_CHECK_CMD:-./scripts/ci.sh}"
FDR_MAX_ITERS="${FDR_MAX_ITERS:-10}"
FDR_ARCHIVE_ARTIFACTS="${FDR_ARCHIVE_ARTIFACTS:-0}"
FDR_COMPACT_ARTIFACTS="${FDR_COMPACT_ARTIFACTS:-1}"
FDR_PR_REVIEW="${FDR_PR_REVIEW:-0}"
FDR_DEBUG="${FDR_DEBUG:-0}"

# Paths
PROJECT_ROOT="$(git rev-parse --show-toplevel 2>/dev/null || pwd)"
FDR_DIR="$PROJECT_ROOT/.fix-die-repeat"
REVIEW_FILE="$FDR_DIR/review.md"
REVIEW_CURRENT_FILE="$FDR_DIR/review_current.md"
BUILD_HISTORY_FILE="$FDR_DIR/build_history.md"
CHECKS_LOG="$FDR_DIR/checks.log"
CHECKS_FILTERED_LOG="$FDR_DIR/checks_filtered.log"
CHECKS_HASH_FILE="$FDR_DIR/.checks_hashes"
PI_LOG="$FDR_DIR/pi.log"
FDR_LOG="$FDR_DIR/fdr.log"
SESSION_LOG=""

# Context limit: If changed files exceed this size (approx 100k tokens),
# we list them in the prompt instead of auto-attaching their content.
# This forces the agent to 'pull' only what it needs.
AUTO_ATTACH_THRESHOLD=$((200 * 1024))  # 200KB

iteration=0
FDR_START_SHA=""
FDR_START_SHA_FILE="$FDR_DIR/.start_sha"

usage() {
    echo "Usage: $(basename "$0") [OPTIONS]"
    echo ""
    echo "Options:"
    echo "  -c, --check-cmd CMD      Command to run checks (default: ./scripts/ci.sh)"
    echo "  -n, --max-iters N        Maximum loop iterations (default: 10)"
    echo "      --archive-artifacts  Archive existing artifacts to a timestamped folder"
    echo "      --no-compact         Skip automatic compaction of large artifacts"
    echo "      --pr-review          Enable PR review mode"
    echo "  -d, --debug              Enable debug mode (timestamped session logs)"
    echo "  -h, --help               Show this help message"
    echo ""
    echo "Environment variables can also be used:"
    echo "  FDR_CHECK_CMD, FDR_MAX_ITERS, FDR_ARCHIVE_ARTIFACTS, FDR_COMPACT_ARTIFACTS, FDR_PR_REVIEW, FDR_DEBUG"
}

log() {
    local message="[$(date '+%Y-%m-%d %H:%M:%S')] [fdr] $*"
    echo "$message"
    if [[ -n "${FDR_LOG:-}" ]]; then
        echo "$message" >> "$FDR_LOG"
    fi
    if [[ -n "${SESSION_LOG:-}" ]]; then
        echo "$message" >> "$SESSION_LOG"
    fi
}

error() {
    local message="[$(date '+%Y-%m-%d %H:%M:%S')] [fdr] ERROR: $*"
    echo "$message" >&2
    if [[ -n "${FDR_LOG:-}" ]]; then
        echo "$message" >> "$FDR_LOG"
    fi
    if [[ -n "${SESSION_LOG:-}" ]]; then
        echo "$message" >> "$SESSION_LOG"
    fi
}

format_duration() {
    local total_seconds=$1
    local hours=$((total_seconds / 3600))
    local minutes=$(((total_seconds % 3600) / 60))
    local seconds=$((total_seconds % 60))

    if [[ $hours -gt 0 ]]; then
        printf "%dh %dm %ds" "$hours" "$minutes" "$seconds"
    elif [[ $minutes -gt 0 ]]; then
        printf "%dm %ds" "$minutes" "$seconds"
    else
        printf "%ds" "$seconds"
    fi
}

append_to_session_log() {
    local title="$1"
    local source_file="$2"

    if [[ -z "${SESSION_LOG:-}" || ! -f "$source_file" ]]; then
        return 0
    fi

    {
        echo ""
        echo "===== $title ====="
        cat "$source_file"
        echo "===== End $title ====="
        echo ""
    } >> "$SESSION_LOG"
}

# Cleanup and logging on unexpected exit
cleanup() {
    local exit_code=$?
    local end_time
    end_time=$(date +%s)

    if [[ -n "${script_start_time:-}" ]]; then
        local duration=$((end_time - script_start_time))
        log "Total run duration: $(format_duration "$duration")"
    fi

    if [[ $exit_code -ne 0 ]]; then
        error "Script exited unexpectedly with code $exit_code at iteration $iteration"
        if [[ -n "$FDR_START_SHA" ]]; then
            error "To see all changes made: git diff $FDR_START_SHA"
            error "To revert all changes:   git checkout $FDR_START_SHA -- ."
        fi
        if [[ -f "$PI_LOG" ]] && [[ -s "$PI_LOG" ]]; then
            error "=== Last 30 lines of pi.log ==="
            tail -30 "$PI_LOG" >&2
            error "=== End of pi.log excerpt ==="
        fi
        if [[ -n "${SESSION_LOG:-}" ]]; then
            error "Full iteration output for this run: $SESSION_LOG"
        fi
    fi
}
trap cleanup EXIT

# ---------------------------------------------------------------------------
# Filter checks.log to extract the most useful failure information.
# Keeps error/warning lines with surrounding context, plus the log tail
# (which typically contains the summary). Writes to CHECKS_FILTERED_LOG.
# ---------------------------------------------------------------------------
filter_checks_log() {
    local max_lines=300
    local context_lines=3
    local tail_lines=80

    if [[ ! -f "$CHECKS_LOG" ]]; then
        return 0
    fi

    local total_lines
    total_lines=$(wc -l < "$CHECKS_LOG")

    # If the log is small enough, just use it directly
    if [[ $total_lines -le $max_lines ]]; then
        cp "$CHECKS_LOG" "$CHECKS_FILTERED_LOG"
        return 0
    fi

    log "Filtering checks.log ($total_lines lines -> ~${max_lines} target)..."

    {
        echo "=== FILTERED CHECK OUTPUT (full log: .fix-die-repeat/checks.log, $total_lines lines) ==="
        echo ""
        echo "--- Error/failure lines with context ---"
        grep -i -n -B "$context_lines" -A "$context_lines" \
            -E '(error[:\[ ]|ERROR[:\[ ]|fatal|FATAL|FAILED|panic|exception|undefined reference|cannot find|no such file|not found|segfault|abort|compilation failed|build failed|assert)' \
            "$CHECKS_LOG" | head -200 || true
        echo ""
        echo "--- Last ${tail_lines} lines ---"
        tail -n "$tail_lines" "$CHECKS_LOG"
    } > "$CHECKS_FILTERED_LOG"

    local filtered_lines
    filtered_lines=$(wc -l < "$CHECKS_FILTERED_LOG")
    log "Filtered checks.log: $total_lines -> $filtered_lines lines"
}

# ---------------------------------------------------------------------------
# Check for oscillation by tracking check output hashes across iterations.
# Prints a warning message if oscillation is detected (empty otherwise).
# ---------------------------------------------------------------------------
check_oscillation() {
    local current_hash="$1"

    if [[ -f "$CHECKS_HASH_FILE" ]]; then
        local prev_match
        prev_match=$(grep "^${current_hash}:" "$CHECKS_HASH_FILE" | tail -1 || true)
        if [[ -n "$prev_match" ]]; then
            local prev_iter
            prev_iter="${prev_match#*:}"
            log "Detected oscillation: iteration $iteration matches iteration $prev_iter"
            echo "WARNING: Check output is IDENTICAL to iteration ${prev_iter}. You are going in CIRCLES. Your previous approach did NOT work — you MUST try a fundamentally DIFFERENT strategy."
        fi
    fi

    # Record this hash
    echo "${current_hash}:${iteration}" >> "$CHECKS_HASH_FILE"
}

# ---------------------------------------------------------------------------
# Fetch PR threads and format them for the agent
# ---------------------------------------------------------------------------
fetch_pr_threads() {
    local branch
    branch=$(git branch --show-current)

    if [[ -z "$branch" ]]; then
        error "Not on a git branch. Skipping PR review."
        return 0
    fi

    log "Fetching PR info for branch: $branch"

    # Check gh auth
    if ! gh auth status >/dev/null 2>&1; then
        error "GitHub CLI not authenticated. Skipping PR review."
        return 0
    fi

    # Get PR info
    local pr_json
    if ! pr_json=$(gh pr view "$branch" --json number,url,headRepository,headRepositoryOwner 2>/dev/null); then
        log "No open PR found for $branch or error fetching PR. Skipping PR review."
        return 0
    fi

    local pr_number pr_url repo_owner repo_name
    pr_number=$(echo "$pr_json" | jq -r .number)
    pr_url=$(echo "$pr_json" | jq -r .url)
    repo_owner=$(echo "$pr_json" | jq -r .headRepositoryOwner.login)
    repo_name=$(echo "$pr_json" | jq -r .headRepository.name)

    log "Found PR #$pr_number ($pr_url). Fetching threads..."

    # GraphQL query to get unresolved threads
    local query='
      query($owner: String!, $repo: String!, $number: Int!) {
        repository(owner: $owner, name: $repo) {
          pullRequest(number: $number) {
            reviewThreads(first: 100) {
              nodes {
                isResolved
                id
                path
                line
                comments(first: 10) {
                  nodes {
                    author { login }
                    body
                  }
                }
              }
            }
          }
        }
      }
    '

    local gql_result
    if ! gql_result=$(gh api graphql -f query="$query" -F owner="$repo_owner" -F repo="$repo_name" -F number="$pr_number"); then
        error "Failed to fetch threads via GraphQL."
        return 0
    fi

    # Parse and format threads using jq
    local threads_output
    threads_output=$(echo "$gql_result" | jq -r '
        .data.repository.pullRequest.reviewThreads.nodes
        | map(select(.isResolved == false))
        | to_entries
        | map(
            "--- Thread #\(.key + 1) ---\n" +
            "ID: \(.value.id)\n" +
            "File: \(.value.path)\n" +
            (if .value.line then "Line: \(.value.line)\n" else "" end) +
            (.value.comments.nodes | map("[\(.author.login // "unknown")]: \(.body)") | join("\n")) +
            "\n"
          )
        | join("\n")
    ')

    if [[ -n "$threads_output" ]]; then
        local count
        count=$(echo "$gql_result" | jq '[.data.repository.pullRequest.reviewThreads.nodes[] | select(.isResolved == false)] | length')

        {
            echo "I've found $count unresolved review threads on PR #$pr_number ($pr_url)."
            echo ""
            echo "Please review each thread, check the associated code, and determine if a fix is required."
            echo "If a fix is needed, apply it. If not, explain why."
            echo ""
            echo "CRITICAL: For each thread below, the 'ID' is the GraphQL thread ID. You MUST use this ID with the available tools:"
            echo " - To resolve a thread after fixing: use 'resolve_pr_threads(threadIds: [\"ID_HERE\"])'"
            echo " - To reply to a thread (e.g. to explain a 'won't fix'): use 'reply_to_thread(threadId: \"ID_HERE\", body: \"...\")' followed by 'resolve_pr_threads(threadIds: [\"ID_HERE\"])'"
            echo ""
            echo "$threads_output"
        } > "$REVIEW_CURRENT_FILE"

        log "Found $count unresolved threads. Added to review queue."
        return 0
    else
        log "No unresolved threads found."
        return 0
    fi
}

# ---------------------------------------------------------------------------
# Compact large persistent artifacts to preserve lessons while reducing noise
# ---------------------------------------------------------------------------
maybe_compact_artifacts() {
    if [[ "$FDR_COMPACT_ARTIFACTS" != "1" && "$FDR_COMPACT_ARTIFACTS" != "true" ]]; then
        return 0
    fi

    local threshold_lines=200
    local needs_compact=0

    for f in "$REVIEW_FILE" "$BUILD_HISTORY_FILE"; do
        if [[ -f "$f" ]] && [[ $(wc -l < "$f") -gt $threshold_lines ]]; then
            needs_compact=1
            break
        fi
    done

    if [[ "$needs_compact" -eq 0 ]]; then
        return 0
    fi

    log "Artifacts exceed ${threshold_lines} lines. Compacting with pi..."

    local compact_args=("-p")
    [[ -f "$REVIEW_FILE" ]] && compact_args+=("@$REVIEW_FILE")
    [[ -f "$BUILD_HISTORY_FILE" ]] && compact_args+=("@$BUILD_HISTORY_FILE")

    # Backup originals
    [[ -f "$REVIEW_FILE" ]] && cp "$REVIEW_FILE" "$REVIEW_FILE.bak"
    [[ -f "$BUILD_HISTORY_FILE" ]] && cp "$BUILD_HISTORY_FILE" "$BUILD_HISTORY_FILE.bak"

    if ! run_pi_safe "${compact_args[@]}" \
        "These are history files from previous fix-die-repeat runs. They have grown large and need compacting.

Your task: read both files and write compacted versions that preserve:
1. Key lessons learned and approaches that FAILED (and why) — these prevent repeated mistakes.
2. Any unresolved or recurring known issues.
3. File paths and areas that have been repeatedly problematic.

Write the compacted review history to '$REVIEW_FILE' and the compacted build history to '$BUILD_HISTORY_FILE'.
Target ~50 lines each. Be concise but do NOT discard failure patterns — those are the most valuable information.
If a file was not provided (doesn't exist), skip it."; then
        log "Compaction failed. Continuing with existing artifacts."
        return 0
    fi

    log "Compaction complete. Backups saved as .bak files in $FDR_DIR."
}

# ---------------------------------------------------------------------------
# Run pi with logging — captures exit code and logs full output
# ---------------------------------------------------------------------------
run_pi() {
    local exit_code=0
    # Capture BOTH stdout and stderr for per-session replay, while preserving
    # normal terminal output streams.
    pi "$@" \
        > >(tee -a "$PI_LOG" "$SESSION_LOG") \
        2> >(tee -a "$PI_LOG" "$SESSION_LOG" >&2) || exit_code=$?

    if [[ $exit_code -ne 0 ]]; then
        error "pi exited with code $exit_code"
        error "pi output logged to: $PI_LOG"
        if [[ -f "$PI_LOG" ]]; then
            error "Last 20 lines of pi.log:"
            tail -20 "$PI_LOG" >&2
        fi
        return $exit_code
    fi
    return 0
}

# ---------------------------------------------------------------------------
# Run pi with a single retry on failure. Returns 0 on success, 1 on double
# failure. Prevents a transient pi crash from killing the entire run.
# ---------------------------------------------------------------------------
run_pi_safe() {
    local exit_code=0
    run_pi "$@" || exit_code=$?

    if [[ $exit_code -eq 0 ]]; then
        return 0
    fi

    # Detect capacity error (503) and trigger model skip if found
    if [[ -f "$PI_LOG" ]] && grep -Ei "503|No capacity" "$PI_LOG" >/dev/null 2>&1; then
        log "Detected model capacity error (503). Skipping current model..."
        # Trigger model skip - this will add the failing model to the persistent cooldown file
        # We use -p to run in print mode
        pi -p "/model-skip" >/dev/null 2>&1 || true
    fi

    log "pi failed (exit $exit_code). Retrying once..."

    exit_code=0
    run_pi "$@" || exit_code=$?

    if [[ $exit_code -eq 0 ]]; then
        return 0
    fi

    error "pi failed twice (exit $exit_code). Continuing loop without pi output for this step."
    return 1
}

# ---------------------------------------------------------------------------
# Get changed (staged + unstaged) files, deduplicated, excluding deleted
# files, .fix-die-repeat/ contents, and large generated files.
# ---------------------------------------------------------------------------

# Patterns for files that should never be included as context
# (lock files, minified assets, etc. - large and not useful for fixing code)
CONTEXT_EXCLUDE_PATTERNS=(
    "*.lock"            # yarn.lock, Cargo.lock, poetry.lock, Gemfile.lock, composer.lock, Pipfile.lock
    "*-lock.json"       # package-lock.json
    "*-lock.yaml"       # pnpm-lock.yaml
    "go.sum"            # Go module checksums
    "*.min.*"           # Minified assets (*.min.js, *.min.css)
)

is_excluded_file() {
    local file="$1"
    local basename
    basename=$(basename "$file")
    for pattern in "${CONTEXT_EXCLUDE_PATTERNS[@]}"; do
        # Use bash pattern matching (extglob not needed for these simple patterns)
        case "$basename" in
            $pattern) return 0 ;;
        esac
    done
    return 1
}

get_changed_files() {
    {
        git diff --name-only 2>/dev/null || true
        git diff --cached --name-only 2>/dev/null || true
        git ls-files --others --exclude-standard 2>/dev/null || true
    } | sort -u | while IFS= read -r file; do
        if [[ -f "$PROJECT_ROOT/$file" ]] && [[ "$file" != .fix-die-repeat/* ]] && ! is_excluded_file "$file"; then
            echo "$file"
        fi
    done
}

# ---------------------------------------------------------------------------
# Collect changed files with size and count limits to avoid context window
# blowout. Prioritizes files mentioned in an optional error log.
# Prints selected filenames to stdout, one per line.
# Usage: read into an array via: mapfile -t arr < <(collect_bounded_context_files ...)
#   or for bash 3.x: while IFS= read -r f; do arr+=("$f"); done < <(...)
# ---------------------------------------------------------------------------
collect_bounded_context_files() {
    local error_log="${1:-}"

    local all_files=()
    while IFS= read -r line; do
        [[ -n "$line" ]] && all_files+=("$line")
    done < <(get_changed_files)

    if [[ ${#all_files[@]} -eq 0 ]]; then
        return 0
    fi

    # If we have an error log, prioritize files mentioned in it
    local prioritized=()
    local rest=()
    if [[ -n "$error_log" ]] && [[ -f "$error_log" ]]; then
        for f in "${all_files[@]}"; do
            if grep -qF "$(basename "$f")" "$error_log" 2>/dev/null; then
                prioritized+=("$f")
            else
                rest+=("$f")
            fi
        done
    else
        rest=("${all_files[@]}")
    fi

    # Output prioritized first, then rest
    if [[ ${#prioritized[@]} -gt 0 ]]; then
        for f in "${prioritized[@]}"; do echo "$f"; done
    fi
    if [[ ${#rest[@]} -gt 0 ]]; then
        for f in "${rest[@]}"; do echo "$f"; done
    fi
}

# ---------------------------------------------------------------------------
# Check for large files (>2000 lines) and generate a warning/recommendation
# ---------------------------------------------------------------------------
detect_large_files_warning() {
    local warning=""
    local found_large=0

    for f in "$@"; do
        if [[ -f "$PROJECT_ROOT/$f" ]]; then
            local lines
            lines=$(wc -l < "$PROJECT_ROOT/$f" | tr -d ' ')
            if [[ "$lines" -gt 2000 ]]; then
                if [[ $found_large -eq 0 ]]; then
                    warning+="\nCRITICAL WARNING: The following files are >2000 lines and will be TRUNCATED by the 'read' tool:\n"
                fi
                warning+="- $f ($lines lines)\n"
                found_large=1
            fi
        fi
    done

    if [[ $found_large -eq 1 ]]; then
        warning+="\n[CRITICAL]: You CANNOT see the bottom of these files. If errors occur there, you are flying blind.\n"
        warning+="STRONGLY RECOMMENDED: Split these files into smaller files or modules to bring them under the 2000-line limit. You cannot reliably fix errors in files you cannot fully read.\n"
        warning+="  - If the file contains tests at the bottom, move them to a separate test file (e.g., tests.rs, test_file.py, file.test.js).\n"
        warning+="  - If it is a large logic file, extract cohesive functionality into separate source files or subfolders.\n"
    fi
    
    echo "$warning"
}

append_review_entry() {
    local timestamp
    timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
    {
        echo "## Iteration $iteration - Review ($timestamp)"
        if [[ -s "$REVIEW_CURRENT_FILE" ]]; then
            cat "$REVIEW_CURRENT_FILE"
        else
            echo "_No issues found._"
        fi
        echo
    } >> "$REVIEW_FILE"
}

append_resolution_entry() {
    local message="$1"
    local timestamp
    timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
    {
        echo "### Iteration $iteration - Resolution ($timestamp)"
        echo "- $message"
        echo
    } >> "$REVIEW_FILE"
}

main() {
    script_start_time=$(date +%s)

    # Argument parsing
    while [[ $# -gt 0 ]]; do
        case "$1" in
            -c|--check-cmd)
                if [[ -n "${2:-}" && ! "$2" =~ ^- ]]; then
                    FDR_CHECK_CMD="$2"
                    shift 2
                else
                    error "Argument for $1 is missing"
                    usage
                    exit 1
                fi
                ;;
            -n|--max-iters)
                if [[ -n "${2:-}" && ! "$2" =~ ^- ]]; then
                    if [[ ! "$2" =~ ^[0-9]+$ ]] || [[ "$2" -le 0 ]]; then
                        error "Invalid value for --max-iters: $2. Must be a positive integer."
                        exit 1
                    fi
                    FDR_MAX_ITERS="$2"
                    shift 2
                else
                    error "Argument for $1 is missing"
                    usage
                    exit 1
                fi
                ;;
            --archive-artifacts)
                FDR_ARCHIVE_ARTIFACTS=1
                shift
                ;;
            --no-compact)
                FDR_COMPACT_ARTIFACTS=0
                shift
                ;;
            --pr-review)
                FDR_PR_REVIEW=1
                shift
                ;;
            -d|--debug)
                FDR_DEBUG=1
                shift
                ;;
            -h|--help)
                usage
                exit 0
                ;;
            *)
                error "Unknown option: $1"
                usage
                exit 1
                ;;
        esac
    done

    # Validate final configuration
    if [[ ! "$FDR_MAX_ITERS" =~ ^[0-9]+$ ]] || [[ "$FDR_MAX_ITERS" -le 0 ]]; then
        error "Invalid configuration: FDR_MAX_ITERS must be a positive integer (got '$FDR_MAX_ITERS')"
        exit 1
    fi

    cd "$PROJECT_ROOT"

    # Ensure .fix-die-repeat directory exists and is gitignored
    mkdir -p "$FDR_DIR"
    if [[ -d "$PROJECT_ROOT/.git" ]]; then
        local gitignore="$PROJECT_ROOT/.gitignore"
        if ! grep -qxF '.fix-die-repeat/' "$gitignore" 2>/dev/null; then
            echo '.fix-die-repeat/' >> "$gitignore"
            log "Added .fix-die-repeat/ to .gitignore"
        fi
    fi

    if [[ "$FDR_ARCHIVE_ARTIFACTS" == "1" || "$FDR_ARCHIVE_ARTIFACTS" == "true" ]]; then
        local timestamp
        timestamp=$(date +%Y%m%d_%H%M%S)
        local archive_dir="$FDR_DIR/archive/$timestamp"
        log "Archiving existing artifacts to $archive_dir"
        mkdir -p "$archive_dir"
        find "$FDR_DIR" -mindepth 1 -maxdepth 1 -type f \( -name "*.log" -o -name "*.md" \) -exec mv -- {} "$archive_dir/" \;
    fi

    # Start a per-run session log that captures all iteration output
    # By default, we use a single session.log to avoid filling the disk.
    # If debug mode is on, we use a timestamped file.
    if [[ "$FDR_DEBUG" == "1" || "$FDR_DEBUG" == "true" ]]; then
        local session_timestamp
        session_timestamp=$(date +%Y%m%d_%H%M%S)
        SESSION_LOG="$FDR_DIR/session_${session_timestamp}.log"
    else
        SESSION_LOG="$FDR_DIR/session.log"
    fi
    > "$SESSION_LOG"
    log "Logging full session output to: $SESSION_LOG"

    # Record starting commit SHA for rollback (stored in a file, not a git tag,
    # to avoid any risk of tags being pushed to remotes)
    if git rev-parse HEAD >/dev/null 2>&1; then
        FDR_START_SHA=$(git rev-parse HEAD)
        echo "$FDR_START_SHA" > "$FDR_START_SHA_FILE"
        log "Git checkpoint: $FDR_START_SHA"
    fi

    # Clear per-run files
    > "$PI_LOG"
    > "$FDR_LOG"
    > "$CHECKS_HASH_FILE"
    log "Logging pi output to: $PI_LOG"
    log "Logging fdr output to: $FDR_LOG"

    # Compact large artifacts from previous runs
    maybe_compact_artifacts

    while true; do
        iteration=$((iteration + 1))
        log "===== Iteration $iteration of $FDR_MAX_ITERS ====="

        if [[ $iteration -gt $FDR_MAX_ITERS ]]; then
            error "Maximum iterations ($FDR_MAX_ITERS) exceeded. Could not resolve all issues."
            if [[ -n "$FDR_START_SHA" ]]; then
                error "To see all changes made: git diff $FDR_START_SHA"
                error "To revert all changes:   git checkout $FDR_START_SHA -- ."
            fi
            exit 1
        fi

        # Step 1: Run checks
        log "[Step 1] Running ${FDR_CHECK_CMD} (output: checks.log)..."
        checks_start_time=$(date +%s)
        if bash -c "$FDR_CHECK_CMD" > "$CHECKS_LOG" 2>&1; then
            checks_status=0
        else
            checks_status=$?
        fi
        checks_end_time=$(date +%s)
        checks_duration=$((checks_end_time - checks_start_time))
        log "[Step 1] run_checks duration: $(format_duration "$checks_duration")"
        append_to_session_log "Iteration $iteration checks output (${FDR_CHECK_CMD})" "$CHECKS_LOG"

        # Check for oscillation (repeated identical failure output)
        current_checks_hash=$(git hash-object "$CHECKS_LOG" 2>/dev/null || echo "iteration_$iteration")
        repeated_failure_warning=""

        if [[ "$checks_status" -ne 0 ]]; then
            repeated_failure_warning=$(check_oscillation "$current_checks_hash")
        fi

        # Step 2: Inner fix loop — if checks failed, keep fixing and re-checking
        # until they pass (or we exhaust the fix attempt budget).
        local fix_attempt=0
        while [[ $checks_status -ne 0 ]]; do
            fix_attempt=$((fix_attempt + 1))

            if [[ $fix_attempt -gt $FDR_MAX_ITERS ]]; then
                error "Maximum fix attempts ($FDR_MAX_ITERS) exhausted in Step 2A. Could not resolve check failures."
                if [[ -n "$FDR_START_SHA" ]]; then
                    error "To see all changes made: git diff $FDR_START_SHA"
                    error "To revert all changes:   git checkout $FDR_START_SHA -- ."
                fi
                exit 1
            fi

            log "[Step 2A] Checks failed (fix attempt $fix_attempt/$FDR_MAX_ITERS). Running pi to fix errors..."

            # Filter the checks log for relevant content
            filter_checks_log

            # Collect context files with limits, prioritizing files in error output
            changed_files=()
            while IFS= read -r f; do
                [[ -n "$f" ]] && changed_files+=("$f")
            done < <(collect_bounded_context_files "$CHECKS_FILTERED_LOG")

            # Construct args for fix step — use filtered log
            fix_checks_args=("-p" "@$CHECKS_FILTERED_LOG")
            if [[ -f "$REVIEW_FILE" ]]; then
                fix_checks_args+=("@$REVIEW_FILE")
            fi
            if [[ -f "$BUILD_HISTORY_FILE" ]]; then
                fix_checks_args+=("@$BUILD_HISTORY_FILE")
            fi

            # Calculate total size of changed files to decide between PUSH (attach) and PULL (list)
            local changed_files_size=0
            if [[ ${#changed_files[@]} -gt 0 ]]; then
                for f in "${changed_files[@]}"; do
                    local s
                    s=$(wc -c < "$PROJECT_ROOT/$f" 2>/dev/null || echo 0)
                    changed_files_size=$((changed_files_size + s))
                done
            fi

            local context_mode="push"
            local large_context_list=""
            
            if [[ $changed_files_size -gt $AUTO_ATTACH_THRESHOLD ]]; then
                context_mode="pull"
                log "Context size (${changed_files_size} bytes) exceeds threshold (${AUTO_ATTACH_THRESHOLD}). Switching to PULL mode (listing files only)."
                
                large_context_list="\n\nThe following files have changed but are too large to pre-load automatically (${changed_files_size} bytes total). You MUST use the 'read' tool to inspect the ones relevant to the error:\n"
                for f in "${changed_files[@]}"; do
                    large_context_list+="- $f\n"
                done
            else
                log "Context size (${changed_files_size} bytes) is within limits. Pushing file contents to prompt."
                for f in "${changed_files[@]}"; do
                    fix_checks_args+=("@$f")
                done
            fi

            # Extract error locations (file:line or file(line,col)) and check for common errors
            local error_locations=""
            local missing_field_hint=""
            
            if [[ -f "$CHECKS_FILTERED_LOG" ]]; then
                # Match both colon format (file:line) and parenthesis format (file(line,col))
                error_locations=$(grep -oE "[a-zA-Z0-9_/.-]+(:[0-9]+|\([0-9]+,[0-9]+\))" "$CHECKS_FILTERED_LOG" 2>/dev/null | head -n 20 | sort -u | tr '\n' ' ' || true)
                
                if grep -q "missing field" "$CHECKS_FILTERED_LOG" 2>/dev/null; then
                     missing_field_hint="HINT: You seem to have a 'missing field' error. You likely updated a struct definition but forgot to update some initializations. Search the codebase for the struct name to find ALL usages (e.g., in tests) and update them."
                fi
            fi

            # Check for large files
            local large_file_warning
            large_file_warning=$(detect_large_files_warning "${changed_files[@]}")

            # Construct prompt for fix step
            fix_checks_prompt="The file .fix-die-repeat/checks_filtered.log contains the failure output from \`${FDR_CHECK_CMD}\` (filtered to error-relevant lines; full log is in .fix-die-repeat/checks.log). "
            if [[ -n "$repeated_failure_warning" ]]; then
                fix_checks_prompt+="$repeated_failure_warning "
            fi
            if [[ -f "$REVIEW_FILE" ]]; then
                 fix_checks_prompt+="I have attached .fix-die-repeat/review.md which contains history of previous iterations. Review it to avoid repeating mistakes. "
            fi
            if [[ -f "$BUILD_HISTORY_FILE" ]]; then
                 fix_checks_prompt+="I have attached .fix-die-repeat/build_history.md which contains a summary of files you modified in previous attempts to fix the build. Use this to avoid repeating ineffective changes. "
            fi
            
            if [[ "$context_mode" == "push" ]]; then
                fix_checks_prompt+="I have also attached the currently changed files for context. "
            else
                fix_checks_prompt+="$large_context_list"
            fi
            
            if [[ -n "$error_locations" ]]; then
                fix_checks_prompt+="\n\nDetected error locations: $error_locations\n"
                fix_checks_prompt+="CRITICAL: If errors are on lines > 2000, you MUST use 'read' with 'offset' to see the code. The 'read' tool truncates large files by default.\n"
            fi
            
            if [[ -n "$missing_field_hint" ]]; then
                fix_checks_prompt+="\n$missing_field_hint\n"
            fi

            if [[ -n "$large_file_warning" ]]; then
                fix_checks_prompt+="$large_file_warning"
            fi
            
            fix_checks_prompt+="Your goal is to FIX the errors. Follow this plan:\n"
            fix_checks_prompt+="1. ANALYZE the log and identify the root cause.\n"
            fix_checks_prompt+="2. PLAN your fix.\n"
            fix_checks_prompt+="3. APPLY the fix using 'edit'.\n"
            fix_checks_prompt+="4. VERIFY with a quick targeted check (e.g., compile the affected file, run the specific failing test). Do NOT run the full \`${FDR_CHECK_CMD}\` — the inner loop will re-run it automatically."

            if ! run_pi_safe "${fix_checks_args[@]}" "$fix_checks_prompt"; then
                log "pi could not produce a fix on attempt $fix_attempt."
            fi

            # Check if any changes were actually made
            if [[ -z "$(git diff --name-only)" && -z "$(git ls-files --others --exclude-standard)" && -z "$(git diff --cached --name-only)" ]]; then
                error "Pi reported success but NO files were modified. This suggests 'edit' commands failed (e.g., text not found)."
                echo "## Iteration $iteration fix attempt $fix_attempt: FAILED to apply fixes (no files changed)" >> "$BUILD_HISTORY_FILE"
            else
                # Record history
                {
                    echo "## Iteration $iteration fix attempt $fix_attempt"
                    git diff --stat
                    echo ""
                } >> "$BUILD_HISTORY_FILE"
            fi

            # Re-run checks immediately to see if the fix worked
            log "[Step 2A] Re-running ${FDR_CHECK_CMD} after fix attempt $fix_attempt..."
            checks_start_time=$(date +%s)
            if bash -c "$FDR_CHECK_CMD" > "$CHECKS_LOG" 2>&1; then
                checks_status=0
            else
                checks_status=$?
            fi
            checks_end_time=$(date +%s)
            checks_duration=$((checks_end_time - checks_start_time))
            log "[Step 2A] re-check duration: $(format_duration "$checks_duration")"
            append_to_session_log "Iteration $iteration fix attempt $fix_attempt re-check (${FDR_CHECK_CMD})" "$CHECKS_LOG"

            # Update oscillation tracking for the new check output
            if [[ $checks_status -ne 0 ]]; then
                current_checks_hash=$(git hash-object "$CHECKS_LOG" 2>/dev/null || echo "fix_attempt_${fix_attempt}")
                repeated_failure_warning=$(check_oscillation "$current_checks_hash")
            else
                repeated_failure_warning=""
            fi
        done

        log "[Step 2B] Checks passed. Proceeding to review."

        # Step 3: Prepare review artifacts
        log "[Step 3] Preparing review artifacts..."
        if [[ ! -f "$REVIEW_FILE" ]]; then
            touch "$REVIEW_FILE"
        fi
        if [[ -f "$REVIEW_CURRENT_FILE" ]]; then
            rm "$REVIEW_CURRENT_FILE"
        fi

        # Step 3.5: Check PR threads if enabled
        if [[ "$FDR_PR_REVIEW" == "1" ]]; then
            log "[Step 3.5] Checking for unresolved PR threads..."
            # Retry if review_current.md is not generated and pi review hasn't run yet
            local pr_fetch_attempt=1
            local max_pr_fetch_attempts=3
            while [[ $pr_fetch_attempt -le $max_pr_fetch_attempts ]]; do
                fetch_pr_threads
                if [[ -s "$REVIEW_CURRENT_FILE" ]]; then
                    break
                fi
                log "PR threads fetch attempt $pr_fetch_attempt produced no issues. Retrying..."
                ((pr_fetch_attempt++))
                sleep 2
            done
            # Note: We keep FDR_PR_REVIEW=1 so it checks every iteration until resolved
        fi

        # Skip local review if we have PR threads to process
        if [[ -s "$REVIEW_CURRENT_FILE" ]]; then
            log "[Step 4] Using PR threads from $REVIEW_CURRENT_FILE for review."
            log "[Step 5] Skipping local file review generation."
        else
            # Step 4: Collect files
            log "[Step 4] Collecting changed and staged files..."
            changed_files=()
            while IFS= read -r f; do
                [[ -n "$f" ]] && changed_files+=("$f")
            done < <(collect_bounded_context_files)

            if [[ ${#changed_files[@]} -eq 0 ]]; then
                log "No changed or staged files found to review. Checks passed. Exiting."
                exit 0
            fi

            log "[Step 4] Found ${#changed_files[@]} file(s) to review"

            # Step 5: Run pi review
            log "[Step 5] Running pi to review files..."
            
            # Generate diff file
            local diff_file="$FDR_DIR/changes.diff"
            {
                if [[ -n "$FDR_START_SHA" ]]; then
                    # Diff from start SHA (covers staged + unstaged changes to tracked files)
                    git diff "$FDR_START_SHA"
                else
                    # Fallback
                    git diff HEAD
                fi
                
                # Append pseudo-diff for untracked files (newly created)
                git ls-files --others --exclude-standard | while read -r f; do
                    if [[ -f "$f" ]] && [[ "$f" != .fix-die-repeat/* ]] && ! is_excluded_file "$f"; then
                         echo "diff --git a/$f b/$f"
                         echo "new file mode 100644"
                         echo "--- /dev/null"
                         echo "+++ b/$f"
                         # simplistic content dump prefixed with +
                         if file "$f" | grep -q "text"; then
                             sed 's/^/+/' "$f"
                         else
                             echo "Binary file $f differs"
                         fi
                         echo "" 
                    fi
                done
            } > "$diff_file"

            local diff_size
            diff_size=$(wc -c < "$diff_file" 2>/dev/null || echo 0)
            log "Generated review diff size: $diff_size bytes"

            # Restrict tools to prevent accidental fixes (no edit/bash), but allow grep/find/ls for exploration
            pi_args=("-p" "--tools" "read,write,grep,find,ls")
            
            # Decide whether to attach the diff or list it based on size
            if [[ $diff_size -gt $AUTO_ATTACH_THRESHOLD ]]; then
                log "Review diff size (${diff_size} bytes) exceeds threshold. Switching to PULL mode."
                local review_prompt_prefix="The changes are too large to attach automatically (${diff_size} bytes). You MUST use the 'read' tool to inspect '.fix-die-repeat/changes.diff'.\n"
            else
                log "Review diff size (${diff_size} bytes) is within limits. Attaching changes.diff."
                pi_args+=("@$diff_file")
                local review_prompt_prefix="I have attached '.fix-die-repeat/changes.diff' which contains the changes made in this session.\n"
            fi

            if [[ -f "$REVIEW_FILE" ]]; then
                pi_args+=("@$REVIEW_FILE")
            fi

            if ! run_pi_safe "${pi_args[@]}" "${review_prompt_prefix}Review the changes for issues. Use .fix-die-repeat/review.md as historical context.
Your task is ONLY to identify and document issues introduced by these changes. Do NOT fix them yourself.
Note: You do not have access to 'edit' or 'bash', so you cannot apply fixes even if you wanted to.

Classify issues as:
- [CRITICAL]: Bugs, security flaws, compilation errors, broken logic.
- [NIT]: Style issues, minor optimizations, comments, formatting.

IMPORTANT:
1. ONLY report [NIT] issues if you also find [CRITICAL] issues.
2. If you only find [NIT] issues (no [CRITICAL] issues), treat this as \"no critical issues found\".
3. If you find [CRITICAL] issues, report BOTH [CRITICAL] and [NIT] issues.

If you find [CRITICAL] issues, use the 'write' tool to save them to '.fix-die-repeat/review_current.md' in the project root.
If you find NO [CRITICAL] issues, use the 'write' tool to create an EMPTY file named '.fix-die-repeat/review_current.md' in the project root.
DO NOT write any text to the file if there are no critical issues — it must be empty."; then
                log "pi review failed. Treating as no issues found."
                touch "$REVIEW_CURRENT_FILE"
            fi
        fi

        append_review_entry

        # Step 6: Analyze review results
        if [[ ! -f "$REVIEW_CURRENT_FILE" ]]; then
            error ".fix-die-repeat/review_current.md was not created by pi. This is unexpected."
            exit 1
        fi

        # Safety net: Detect if review_current.md contains only "no issues" messages
        # This handles cases where pi writes "No critical issues found" instead of an empty file
        local review_file_is_empty=0
        if [[ -s "$REVIEW_CURRENT_FILE" ]]; then
            if grep -qi "No critical issues found" "$REVIEW_CURRENT_FILE" 2>/dev/null; then
                # Count non-header, non-empty lines to verify there's no actual content
                local content_lines
                content_lines=$(grep -v "^#" "$REVIEW_CURRENT_FILE" | grep -v "^$" | wc -l | tr -d ' ')
                if [[ "$content_lines" -le 1 ]]; then
                    log "review_current.md contains only 'no issues' message (${content_lines} content lines). Treating as empty."
                    review_file_is_empty=1
                    # Truncate to empty file to avoid confusion in future iterations
                    > "$REVIEW_CURRENT_FILE"
                fi
            fi
        fi

        if [[ -s "$REVIEW_CURRENT_FILE" ]]; then
            log "[Step 6A] Issues found in .fix-die-repeat/review_current.md. Running pi to fix them..."

            local fix_attempt=1
            local max_fix_attempts=3
            while [[ $fix_attempt -le $max_fix_attempts ]]; do
                log "[Step 6A] Pi fix attempt $fix_attempt of $max_fix_attempts..."
                
                fix_args=("-p" "@$REVIEW_CURRENT_FILE")
                if [[ -f "$REVIEW_FILE" ]]; then
                    fix_args+=("@$REVIEW_FILE")
                fi

                # Extract file paths from review_current.md and attach them as context
                # This helps pi see the files it needs to fix.
                while IFS= read -r line; do
                    if [[ "$line" =~ ^File:[[:space:]]*(.+) ]]; then
                        local context_file="${BASH_REMATCH[1]}"
                        if [[ -f "$context_file" ]]; then
                            fix_args+=("@$context_file")
                        fi
                    fi
                done < "$REVIEW_CURRENT_FILE"

                if ! run_pi_safe "${fix_args[@]}" "Fix all issues documented in .fix-die-repeat/review_current.md. Address each issue mentioned in the file.

Follow this plan:
1. READ the issues in .fix-die-repeat/review_current.md.
2. PLAN your fixes.
3. If the issue is testable, CREATE a unit test to prevent regression.
4. APPLY the fixes using 'edit'.
5. VERIFY with a quick targeted check (e.g., compile the affected file, run the specific failing test). Do NOT run the full \`${FDR_CHECK_CMD}\` — the outer loop will do that.

CRITICAL: You MUST NOT commit your changes at any point. The script handles git operations.

Ensure that your changes do not re-surface older issues listed in .fix-die-repeat/review.md.
If these issues are PR threads (indicated by 'Thread #ID'), use the 'resolve_pr_threads' tool to mark them resolved on GitHub ONLY after verifying the fix."; then
                    log "pi fix failed on attempt $fix_attempt."
                fi
                append_resolution_entry "Fixes applied for .fix-die-repeat/review_current.md (attempt $fix_attempt); verification pending."

                # Check if any changes were actually made
                if [[ -z "$(git diff --name-only)" && -z "$(git ls-files --others --exclude-standard)" && -z "$(git diff --cached --name-only)" ]]; then
                    error "Pi reported success on attempt $fix_attempt but NO files were modified. This suggests 'edit' commands failed (e.g., text not found)."
                    echo "## Iteration $iteration fix attempt $fix_attempt: FAILED to apply fixes (no files changed)" >> "$BUILD_HISTORY_FILE"
                    
                    if [[ $fix_attempt -lt $max_fix_attempts ]]; then
                        log "Retrying fix (attempt $((fix_attempt + 1)))..."
                        ((fix_attempt++))
                        continue
                    fi
                else
                    # Record history for review-fix changes
                    {
                        echo "## Iteration $iteration Review Fixes (attempt $fix_attempt)"
                        git diff --stat
                        echo ""
                    } >> "$BUILD_HISTORY_FILE"
                    break # Success!
                fi
                ((fix_attempt++))
            done

            continue
        else
            log "[Step 6B] No issues found in .fix-die-repeat/review_current.md."
            append_resolution_entry "No issues found."
        fi

        # Step 7: Done
        if [[ -f "$REVIEW_CURRENT_FILE" ]]; then
            rm "$REVIEW_CURRENT_FILE"
        fi
        # Clean up checkpoint file on success
        rm -f "$FDR_START_SHA_FILE"
        log "[Step 7] Done! All checks passed and no review issues found. .fix-die-repeat/review.md retained. Session log: $SESSION_LOG"
        exit 0
    done
}

main "$@"
