#!/usr/bin/env bash
#
# fix-die-repeat - Automated check, review, and fix loop using pi
#
# Usage: fix-die-repeat [OPTIONS]
#
# Options:
#   -c, --check-cmd CMD      Command to run checks (default: ./scripts/ci.sh)
#   -n, --max-iters N        Maximum loop iterations (default: 10)
#   -m, --model MODEL        Override model selection (e.g., anthropic/claude-sonnet-4-5)
#       --archive-artifacts  Archive existing artifacts to a timestamped folder
#       --no-compact         Skip automatic compaction of large artifacts
#       --pr-review          Enable PR review mode
#   -d, --debug              Enable debug mode (timestamped session logs)
#   -h, --help               Show this help message
#
# Environment variables:
#   FDR_CHECK_CMD, FDR_MAX_ITERS, FDR_MODEL, FDR_ARCHIVE_ARTIFACTS, FDR_COMPACT_ARTIFACTS, FDR_PR_REVIEW, FDR_DEBUG
#

set -euo pipefail

# Defaults (can be overridden by env vars)
FDR_CHECK_CMD="${FDR_CHECK_CMD:-./scripts/ci.sh}"
FDR_MAX_ITERS="${FDR_MAX_ITERS:-10}"
FDR_MODEL="${FDR_MODEL:-}"
FDR_TEST_MODEL="${FDR_TEST_MODEL:-}"
FDR_MAX_PR_THREADS="${FDR_MAX_PR_THREADS:-5}"
FDR_ARCHIVE_ARTIFACTS="${FDR_ARCHIVE_ARTIFACTS:-0}"
FDR_COMPACT_ARTIFACTS="${FDR_COMPACT_ARTIFACTS:-1}"
FDR_PR_REVIEW="${FDR_PR_REVIEW:-0}"
FDR_DEBUG="${FDR_DEBUG:-0}"

# Paths
PROJECT_ROOT="$(git rev-parse --show-toplevel 2>/dev/null || pwd)"
FDR_DIR="$PROJECT_ROOT/.fix-die-repeat"
REVIEW_FILE="$FDR_DIR/review.md"
REVIEW_CURRENT_FILE="$FDR_DIR/review_current.md"
BUILD_HISTORY_FILE="$FDR_DIR/build_history.md"
CHECKS_LOG="$FDR_DIR/checks.log"
CHECKS_FILTERED_LOG="$FDR_DIR/checks_filtered.log"
CHECKS_HASH_FILE="$FDR_DIR/.checks_hashes"
PI_LOG="$FDR_DIR/pi.log"
FDR_LOG="$FDR_DIR/fdr.log"
PR_THREADS_CACHE="$FDR_DIR/.pr_threads_cache"
PR_THREADS_HASH_FILE="$FDR_DIR/.pr_threads_hash"
SESSION_LOG=""

# Context limit: If changed files exceed this size (approx 100k tokens),
# we list them in the prompt instead of auto-attaching their content.
# This forces the agent to 'pull' only what it needs.
AUTO_ATTACH_THRESHOLD=$((200 * 1024))  # 200KB

iteration=0
FDR_START_SHA=""
FDR_START_SHA_FILE="$FDR_DIR/.start_sha"

# Track no-progress detection for PR review mode
pr_review_no_progress_count=0

# Track toolless responses (no file edits across attempts)
consecutive_toolless_attempts=0

# Track iteration state for no-progress detection
last_review_current_hash=""
last_git_state=""

# Track PR thread IDs for safe resolution (prevent accidentally resolving unhandled threads)
pr_thread_ids_file="$FDR_DIR/.pr_thread_ids_in_scope"
pr_resolved_threads_file="$FDR_DIR/.resolved_threads"

# Keep a minimum delay between sequential pi invocations to reduce lock
# contention when wrapper flows trigger multiple calls back-to-back.
PI_SEQUENTIAL_DELAY_SECONDS=1
pi_invocation_count=0

before_pi_call() {
    if [[ $pi_invocation_count -gt 0 ]]; then
        sleep "$PI_SEQUENTIAL_DELAY_SECONDS"
    fi
    pi_invocation_count=$((pi_invocation_count + 1))
}

# ---------------------------------------------------------------------------
# Extract PR thread IDs from review_current.md for safe resolution tracking
# ---------------------------------------------------------------------------
extract_pr_thread_ids() {
    if [[ ! -f "$REVIEW_CURRENT_FILE" ]]; then
        return 0
    fi

    rg "^ID: " "$REVIEW_CURRENT_FILE" | sed 's/^ID: //' > "$pr_thread_ids_file"
    local count
    count=$(wc -l < "$pr_thread_ids_file" | tr -d ' ')
    log "Extracted $count PR thread IDs from review_current.md"
    debug_log "Thread IDs extracted: $(cat "$pr_thread_ids_file" 2>/dev/null | tr '\n' ' ')"
}

# ---------------------------------------------------------------------------
# Call pi's resolve_pr_threads tool with specific thread IDs
# ---------------------------------------------------------------------------
resolve_pr_threads() {
    local thread_ids="$1"

    if [[ -z "$thread_ids" ]]; then
        log "No thread IDs to resolve."
        return 0
    fi

    # Format thread IDs as JSON array for pi
    local ids_json
    ids_json=$(echo "$thread_ids" | tr '\n' ',' | sed 's/^/["/' | sed 's/$/"]/')

    log "Resolving PR threads via pi..."

    before_pi_call
    if pi -p "resolve_pr_threads(threadIds: [$ids_json])"; then
        return 0
    else
        log "Warning: Failed to resolve PR threads."
        return 1
    fi
}

# ---------------------------------------------------------------------------
# Enhanced debug logging function
# ---------------------------------------------------------------------------
debug_log() {
    if [[ "$FDR_DEBUG" != "1" && "$FDR_DEBUG" != "true" ]]; then
        return 0
    fi

    log "[DEBUG] $*"
}

# ---------------------------------------------------------------------------
# Test model compatibility before running full loop
# ---------------------------------------------------------------------------
test_model() {
    if [[ -z "$FDR_TEST_MODEL" ]]; then
        log "No --test-model specified, skipping test."
        return 0
    fi

    local test_file="$FDR_DIR/.model_test_result.txt"

    log "===== Testing model compatibility: $FDR_TEST_MODEL ====="
    log "Running simple write test to verify model can use pi's tools..."

    # Create test prompt - ask model to write a simple string
    before_pi_call
    if ! pi -p --model "$FDR_TEST_MODEL" "Write 'MODEL TEST OK' to file $test_file. Do NOT use any other tools or generate pseudo-code." > "$PI_LOG" 2>&1; then
        log "pi test invocation failed with code $?"
        error "Model $FDR_TEST_MODEL failed basic invocation test."
        rm -f "$test_file" 2>/dev/null || true
        return 1
    fi

    # Check if model wrote the expected output
    if [[ -f "$test_file" ]] && grep -q "MODEL TEST OK" "$test_file"; then
        log "Model $FDR_TEST_MODEL PASSED tool test."
        log "Test output: $(cat "$test_file" 2>/dev/null)"

        # Clean up test file
        rm -f "$test_file" 2>/dev/null || true

        # Show model information
        log "Model is compatible for code editing. Ready to proceed."
        log ""
        log "To run with this model: fix-die-repeat --model $FDR_TEST_MODEL"
        log "Or set via env var: export FDR_MODEL=$FDR_TEST_MODEL"
        exit 0
    else
        local test_output
        test_output=$(cat "$test_file" 2>/dev/null || echo "(empty)")
        log "Model $FDR_TEST_MODEL FAILED tool test."
        log "Test output: $test_output"

        # Check for pseudo-code or hallucinated tools
        if rg -qi "print|IO\.puts|System\.cmd|File\.write|defmodule" "$test_file" 2>/dev/null; then
            log "WARNING: Model generated pseudo-code instead of using pi's tools."
            log "This model appears incompatible with pi's tool-calling interface."
        fi

        # Clean up test file
        rm -f "$test_file" 2>/dev/null || true

        log "Model $FDR_TEST_MODEL is NOT suitable for code editing tasks."
        log ""
        log "RECOMMENDATION: Try a different model:"
        log "  - anthropic/claude-sonnet-4-5 (recommended for code editing)"
        log "  - anthropic/claude-opus-4-6 (high capacity, more expensive)"
        log "  - github-copilot/gpt-5.2-codex (good for code generation)"
        log ""
        log "To override: fix-die-repeat --model <model>"
        exit 1
    fi
}

usage() {
    echo "Usage: $(basename "$0") [OPTIONS]"
    echo ""
    echo "Options:"
    echo "  -c, --check-cmd CMD      Command to run checks (default: ./scripts/ci.sh)"
    echo "  -n, --max-iters N        Maximum loop iterations (default: 10)"
    echo "  -m, --model MODEL        Override model selection (e.g., anthropic/claude-sonnet-4-5)"
    echo "      --max-pr-threads N   Maximum PR threads to process per iteration (default: 5)"
    echo "      --archive-artifacts  Archive existing artifacts to a timestamped folder"
    echo "      --no-compact         Skip automatic compaction of large artifacts"
    echo "      --pr-review          Enable PR review mode"
    echo "      --test-model MODEL    Test model compatibility before running (exits after test)"
    echo "  -d, --debug              Enable debug mode (timestamped session logs and verbose logging)"
    echo "  -h, --help               Show this help message"
    echo ""
    echo "Environment variables can also be used:"
    echo "  FDR_CHECK_CMD, FDR_MAX_ITERS, FDR_MODEL, FDR_MAX_PR_THREADS, FDR_TEST_MODEL, FDR_ARCHIVE_ARTIFACTS, FDR_COMPACT_ARTIFACTS, FDR_PR_REVIEW, FDR_DEBUG"
}

log() {
    local message="[$(date '+%Y-%m-%d %H:%M:%S')] [fdr] $*"
    echo "$message"
    if [[ -n "${FDR_LOG:-}" ]]; then
        echo "$message" >> "$FDR_LOG"
    fi
    if [[ -n "${SESSION_LOG:-}" ]]; then
        echo "$message" >> "$SESSION_LOG"
    fi
}

error() {
    local message="[$(date '+%Y-%m-%d %H:%M:%S')] [fdr] ERROR: $*"
    echo "$message" >&2
    if [[ -n "${FDR_LOG:-}" ]]; then
        echo "$message" >> "$FDR_LOG"
    fi
    if [[ -n "${SESSION_LOG:-}" ]]; then
        echo "$message" >> "$SESSION_LOG"
    fi
}

format_duration() {
    local total_seconds=$1
    local hours=$((total_seconds / 3600))
    local minutes=$(((total_seconds % 3600) / 60))
    local seconds=$((total_seconds % 60))

    if [[ $hours -gt 0 ]]; then
        printf "%dh %dm %ds" "$hours" "$minutes" "$seconds"
    elif [[ $minutes -gt 0 ]]; then
        printf "%dm %ds" "$minutes" "$seconds"
    else
        printf "%ds" "$seconds"
    fi
}

append_to_session_log() {
    local title="$1"
    local source_file="$2"

    if [[ -z "${SESSION_LOG:-}" || ! -f "$source_file" ]]; then
        return 0
    fi

    {
        echo ""
        echo "===== $title ====="
        cat "$source_file"
        echo "===== End $title ====="
        echo ""
    } >> "$SESSION_LOG"
}

# Cleanup and logging on unexpected exit
cleanup() {
    local exit_code=$?
    local end_time
    end_time=$(date +%s)

    if [[ -n "${script_start_time:-}" ]]; then
        local duration=$((end_time - script_start_time))
        log "Total run duration: $(format_duration "$duration")"
    fi

    if [[ $exit_code -ne 0 ]]; then
        error "Script exited unexpectedly with code $exit_code at iteration $iteration"
        if [[ -n "$FDR_START_SHA" ]]; then
            error "To see all changes made: git diff $FDR_START_SHA"
            error "To revert all changes:   git checkout $FDR_START_SHA -- ."
        fi
        if [[ -f "$PI_LOG" ]] && [[ -s "$PI_LOG" ]]; then
            error "=== Last 30 lines of pi.log ==="
            tail -30 "$PI_LOG" >&2
            error "=== End of pi.log excerpt ==="
        fi
        if [[ -n "${SESSION_LOG:-}" ]]; then
            error "Full iteration output for this run: $SESSION_LOG"
        fi
    fi
}
trap cleanup EXIT

# ---------------------------------------------------------------------------
# Filter checks.log to extract the most useful failure information.
# Keeps error/warning lines with surrounding context, plus the log tail
# (which typically contains the summary). Writes to CHECKS_FILTERED_LOG.
# ---------------------------------------------------------------------------
filter_checks_log() {
    local max_lines=300
    local context_lines=3
    local tail_lines=80

    if [[ ! -f "$CHECKS_LOG" ]]; then
        return 0
    fi

    local total_lines
    total_lines=$(wc -l < "$CHECKS_LOG")

    # If the log is small enough, just use it directly
    if [[ $total_lines -le $max_lines ]]; then
        cp "$CHECKS_LOG" "$CHECKS_FILTERED_LOG"
        return 0
    fi

    log "Filtering checks.log ($total_lines lines -> ~${max_lines} target)..."

    {
        echo "=== FILTERED CHECK OUTPUT (full log: .fix-die-repeat/checks.log, $total_lines lines) ==="
        echo ""
        echo "--- Error/failure lines with context ---"
        grep -i -n -B "$context_lines" -A "$context_lines" \
            -E '(error[:\[ ]|ERROR[:\[ ]|fatal|FATAL|FAILED|panic|exception|undefined reference|cannot find|no such file|not found|segfault|abort|compilation failed|build failed|assert)' \
            "$CHECKS_LOG" | head -200 || true
        echo ""
        echo "--- Last ${tail_lines} lines ---"
        tail -n "$tail_lines" "$CHECKS_LOG"
    } > "$CHECKS_FILTERED_LOG"

    local filtered_lines
    filtered_lines=$(wc -l < "$CHECKS_FILTERED_LOG")
    log "Filtered checks.log: $total_lines -> $filtered_lines lines"
}

# ---------------------------------------------------------------------------
# Check for oscillation by tracking check output hashes across iterations.
# Prints a warning message if oscillation is detected (empty otherwise).
# ---------------------------------------------------------------------------
check_oscillation() {
    local current_hash="$1"

    if [[ -f "$CHECKS_HASH_FILE" ]]; then
        local prev_match
        prev_match=$(grep "^${current_hash}:" "$CHECKS_HASH_FILE" | tail -1 || true)
        if [[ -n "$prev_match" ]]; then
            local prev_iter
            prev_iter="${prev_match#*:}"
            log "Detected oscillation: iteration $iteration matches iteration $prev_iter"
            echo "WARNING: Check output is IDENTICAL to iteration ${prev_iter}. You are going in CIRCLES. Your previous approach did NOT work — you MUST try a fundamentally DIFFERENT strategy."
        fi
    fi

    # Record this hash
    echo "${current_hash}:${iteration}" >> "$CHECKS_HASH_FILE"
}

# ---------------------------------------------------------------------------
# Fetch PR threads and format them for the agent
# ---------------------------------------------------------------------------
fetch_pr_threads() {
    local branch
    branch=$(git branch --show-current)

    if [[ -z "$branch" ]]; then
        error "Not on a git branch. Skipping PR review."
        return 0
    fi

    log "Fetching PR info for branch: $branch"

    # Check gh auth
    if ! gh auth status >/dev/null 2>&1; then
        error "GitHub CLI not authenticated. Skipping PR review."
        return 0
    fi

    # Get PR info
    local pr_json
    if ! pr_json=$(gh pr view "$branch" --json number,url,headRepository,headRepositoryOwner 2>/dev/null); then
        log "No open PR found for $branch or error fetching PR. Skipping PR review."
        return 0
    fi

    local pr_number pr_url repo_owner repo_name
    pr_number=$(echo "$pr_json" | jq -r .number)
    pr_url=$(echo "$pr_json" | jq -r .url)
    repo_owner=$(echo "$pr_json" | jq -r .headRepositoryOwner.login)
    repo_name=$(echo "$pr_json" | jq -r .headRepository.name)

    log "Found PR #$pr_number ($pr_url). Checking for cached threads..."

    # Check cache for PR threads to avoid unnecessary refetches
    if [[ -f "$PR_THREADS_CACHE" ]] && [[ -f "$PR_THREADS_HASH_FILE" ]]; then
        local cached_hash
        cached_hash=$(cat "$PR_THREADS_HASH_FILE" 2>/dev/null || echo "no_cache")

        # Compute hash based on PR number and repo to detect changes
        local pr_key
        pr_key="${repo_owner}/${repo_name}/${pr_number}"

        if [[ "$cached_hash" == "$pr_key" ]]; then
            log "Using cached PR threads (unchanged)..."
            cp "$PR_THREADS_CACHE" "$REVIEW_CURRENT_FILE"
            local cached_count
            cached_count=$(rg -c "^--- Thread #" "$REVIEW_CURRENT_FILE" 2>/dev/null || echo 0)
            log "Found $cached_count unresolved threads from cache."
            return 0
        fi
    fi

    log "Cache miss or invalid. Fetching fresh threads..."

    # GraphQL query to get unresolved threads
    local query='
      query($owner: String!, $repo: String!, $number: Int!) {
        repository(owner: $owner, name: $repo) {
          pullRequest(number: $number) {
            reviewThreads(first: 100) {
              nodes {
                isResolved
                id
                path
                line
                comments(first: 10) {
                  nodes {
                    author { login }
                    body
                  }
                }
              }
            }
          }
        }
      }
    '

    local gql_result
    if ! gql_result=$(gh api graphql -f query="$query" -F owner="$repo_owner" -F repo="$repo_name" -F number="$pr_number"); then
        error "Failed to fetch threads via GraphQL."
        return 0
    fi

    # Parse and format threads using jq
    local threads_output
    threads_output=$(echo "$gql_result" | jq -r '
        .data.repository.pullRequest.reviewThreads.nodes
        | map(select(.isResolved == false))
        | to_entries
        | map(
            "--- Thread #\(.key + 1) ---\n" +
            "ID: \(.value.id)\n" +
            "File: \(.value.path)\n" +
            (if .value.line then "Line: \(.value.line)\n" else "" end) +
            (.value.comments.nodes | map("[\(.author.login // "unknown")]: \(.body)") | join("\n")) +
            "\n"
          )
        | join("\n")
    ')

    if [[ -n "$threads_output" ]]; then
        local count
        count=$(echo "$gql_result" | jq '[.data.repository.pullRequest.reviewThreads.nodes[] | select(.isResolved == false)] | length')

        {
            echo "I've found $count unresolved review threads on PR #$pr_number ($pr_url)."
            echo ""
            echo "Please review each thread, check the associated code, and determine if a fix is required."
            echo "If a fix is needed, apply it. If not, explain why."
            echo ""
            echo "CRITICAL: For each thread below, the 'ID' is the GraphQL thread ID. You MUST use this ID with the available tools:"
            echo " - To resolve a thread after fixing: use 'resolve_pr_threads(threadIds: [\"ID_HERE\"])'"
            echo " - To reply to a thread (e.g. to explain a 'won't fix'): use 'reply_to_thread(threadId: \"ID_HERE\", body: \"...\")' followed by 'resolve_pr_threads(threadIds: [\"ID_HERE\"])'"
            echo ""
            echo "$threads_output"
        } > "$REVIEW_CURRENT_FILE"

        log "Found $count unresolved threads. Added to review queue."

        # Cache the fetched threads
        cp "$REVIEW_CURRENT_FILE" "$PR_THREADS_CACHE"
        # Store hash key: repo/owner/number
        local pr_key="${repo_owner}/${repo_name}/${pr_number}"
        echo "$pr_key" > "$PR_THREADS_HASH_FILE"

        return 0
    else
        log "No unresolved threads found."
        return 0
    fi
}

# ---------------------------------------------------------------------------
# Check and compact large persistent artifacts to preserve lessons while reducing noise
# ---------------------------------------------------------------------------
check_and_compact_artifacts() {
    if [[ "$FDR_COMPACT_ARTIFACTS" != "1" && "$FDR_COMPACT_ARTIFACTS" != "true" ]]; then
        return 0
    fi

    local compact_threshold_lines=150
    local emergency_threshold_lines=200
    local needs_compact=0
    local needs_emergency=0

    for f in "$REVIEW_FILE" "$BUILD_HISTORY_FILE"; do
        if [[ -f "$f" ]]; then
            local line_count
            line_count=$(wc -l < "$f" | tr -d ' ')

            if [[ $line_count -gt $emergency_threshold_lines ]]; then
                needs_emergency=1
                break
            elif [[ $line_count -gt $compact_threshold_lines ]]; then
                needs_compact=1
            fi
        fi
    done

    if [[ $needs_emergency -eq 1 ]]; then
        log "Emergency: artifacts exceed ${emergency_threshold_lines} lines. Truncating to last 100 lines..."
        [[ -f "$REVIEW_FILE" ]] && tail -n 100 "$REVIEW_FILE" > "$REVIEW_FILE.tmp" && mv "$REVIEW_FILE.tmp" "$REVIEW_FILE"
        [[ -f "$BUILD_HISTORY_FILE" ]] && tail -n 100 "$BUILD_HISTORY_FILE" > "$BUILD_HISTORY_FILE.tmp" && mv "$BUILD_HISTORY_FILE.tmp" "$BUILD_HISTORY_FILE"
        return 0
    fi

    if [[ $needs_compact -eq 0 ]]; then
        return 0
    fi

    log "Artifacts exceed ${compact_threshold_lines} lines. Compacting with pi..."

    local compact_args=("-p")
    [[ -f "$REVIEW_FILE" ]] && compact_args+=("@$REVIEW_FILE")
    [[ -f "$BUILD_HISTORY_FILE" ]] && compact_args+=("@$BUILD_HISTORY_FILE")

    # Backup originals
    [[ -f "$REVIEW_FILE" ]] && cp "$REVIEW_FILE" "$REVIEW_FILE.bak"
    [[ -f "$BUILD_HISTORY_FILE" ]] && cp "$BUILD_HISTORY_FILE" "$BUILD_HISTORY_FILE.bak"

    if ! run_pi_safe "${compact_args[@]}" \
        "These are history files from previous fix-die-repeat runs. They have grown large and need compacting to stay within Opus 4.6's ~32K token context limit.

Your task: read both files and write compacted versions that preserve ONLY:

1. Key lessons learned and approaches that FAILED (and why) — these prevent repeated mistakes.
2. Files/areas that have been repeatedly problematic.

REMOVE all of the following:
- Successful resolution entries (they don't help avoid mistakes)
- Iteration numbers and timestamps (noise)
- Generic \"no issues found\" entries
- Identical PR thread descriptions from multiple iterations

Write the compacted review history to '$REVIEW_FILE' and the compacted build history to '$BUILD_HISTORY_FILE'.
Target **30-35 lines** each.
If a file was not provided (doesn't exist), skip it."; then
        log "Compaction failed. Continuing with existing artifacts."
        return 0
    fi

    # Verify compaction actually reduced file sizes
    for f in "$REVIEW_FILE" "$BUILD_HISTORY_FILE"; do
        if [[ -f "$f.bak" ]]; then
            local before_lines
            before_lines=$(wc -l < "$f.bak" 2>/dev/null || echo 0)
            local after_lines
            after_lines=$(wc -l < "$f" 2>/dev/null || echo 0)

            if [[ $after_lines -ge $before_lines ]]; then
                log "Warning: Compaction failed to reduce size of $f (${before_lines} → ${after_lines} lines). Force truncating to 50 lines..."
                tail -n 50 "$f.bak" > "$f" 2>/dev/null || true
            else
                log "Compaction reduced $f from ${before_lines} to ${after_lines} lines."
            fi
        fi
    done

    log "Compaction complete. Backups saved as .bak files in $FDR_DIR."
}

# ---------------------------------------------------------------------------
# Compact large persistent artifacts to preserve lessons while reducing noise
# ---------------------------------------------------------------------------
maybe_compact_artifacts() {
    check_and_compact_artifacts
}

# ---------------------------------------------------------------------------
# Run pi with logging — captures exit code and logs full output
# ---------------------------------------------------------------------------
run_pi() {
    local exit_code=0
    # Capture BOTH stdout and stderr for per-session replay, while preserving
    # normal terminal output streams.
    before_pi_call
    pi "$@" \
        > >(tee -a "$PI_LOG" "$SESSION_LOG") \
        2> >(tee -a "$PI_LOG" "$SESSION_LOG" >&2) || exit_code=$?

    if [[ $exit_code -ne 0 ]]; then
        error "pi exited with code $exit_code"
        error "pi output logged to: $PI_LOG"
        if [[ -f "$PI_LOG" ]]; then
            error "Last 20 lines of pi.log:"
            tail -20 "$PI_LOG" >&2
        fi
        return $exit_code
    fi
    return 0
}

# ---------------------------------------------------------------------------
# Run pi with a single retry on failure. Returns 0 on success, 1 on double
# failure. Prevents a transient pi crash from killing the entire run.
# ---------------------------------------------------------------------------
run_pi_safe() {
    local exit_code=0
    run_pi "$@" || exit_code=$?

    if [[ $exit_code -eq 0 ]]; then
        return 0
    fi

    # Detect capacity error (503) and trigger model skip if found
    if [[ -f "$PI_LOG" ]] && grep -Ei "503|No capacity" "$PI_LOG" >/dev/null 2>&1; then
        log "Detected model capacity error (503). Skipping current model..."
        # Trigger model skip - this will add the failing model to the persistent cooldown file
        # We use -p to run in print mode
        before_pi_call
        pi -p "/model-skip" >/dev/null 2>&1 || true
    fi

    # Detect long context error (429) and force compaction before retry
    if [[ -f "$PI_LOG" ]] && grep -qi "429.*long context" "$PI_LOG" >/dev/null 2>&1; then
        log "Detected long context rate limit (429). Forcing emergency compaction..."
        # Force emergency truncation of artifacts
        [[ -f "$REVIEW_FILE" ]] && tail -n 100 "$REVIEW_FILE" > "$REVIEW_FILE.tmp" && mv "$REVIEW_FILE.tmp" "$REVIEW_FILE"
        [[ -f "$BUILD_HISTORY_FILE" ]] && tail -n 100 "$BUILD_HISTORY_FILE" > "$BUILD_HISTORY_FILE.tmp" && mv "$BUILD_HISTORY_FILE.tmp" "$BUILD_HISTORY_FILE"
        log "Emergency compaction complete. Retrying..."
    fi

    log "pi failed (exit $exit_code). Retrying once..."

    exit_code=0
    run_pi "$@" || exit_code=$?

    if [[ $exit_code -eq 0 ]]; then
        return 0
    fi

    error "pi failed twice (exit $exit_code). Continuing loop without pi output for this step."
    return 1
}

# ---------------------------------------------------------------------------
# Get changed (staged + unstaged) files, deduplicated, excluding deleted
# files, .fix-die-repeat/ contents, and large generated files.
# ---------------------------------------------------------------------------

# Patterns for files that should never be included as context
# (lock files, minified assets, etc. - large and not useful for fixing code)
CONTEXT_EXCLUDE_PATTERNS=(
    "*.lock"            # yarn.lock, Cargo.lock, poetry.lock, Gemfile.lock, composer.lock, Pipfile.lock
    "*-lock.json"       # package-lock.json
    "*-lock.yaml"       # pnpm-lock.yaml
    "go.sum"            # Go module checksums
    "*.min.*"           # Minified assets (*.min.js, *.min.css)
)

is_excluded_file() {
    local file="$1"
    local basename
    basename=$(basename "$file")
    for pattern in "${CONTEXT_EXCLUDE_PATTERNS[@]}"; do
        # Use bash pattern matching (extglob not needed for these simple patterns)
        case "$basename" in
            $pattern) return 0 ;;
        esac
    done
    return 1
}

get_changed_files() {
    {
        git diff --name-only 2>/dev/null || true
        git diff --cached --name-only 2>/dev/null || true
        git ls-files --others --exclude-standard 2>/dev/null || true
    } | sort -u | while IFS= read -r file; do
        if [[ -f "$PROJECT_ROOT/$file" ]] && [[ "$file" != .fix-die-repeat/* ]] && ! is_excluded_file "$file"; then
            echo "$file"
        fi
    done
}

# ---------------------------------------------------------------------------
# Collect changed files with size and count limits to avoid context window
# blowout. Prioritizes files mentioned in an optional error log.
# Prints selected filenames to stdout, one per line.
# Usage: read into an array via: mapfile -t arr < <(collect_bounded_context_files ...)
#   or for bash 3.x: while IFS= read -r f; do arr+=("$f"); done < <(...)
# ---------------------------------------------------------------------------
collect_bounded_context_files() {
    local error_log="${1:-}"

    local all_files=()
    while IFS= read -r line; do
        [[ -n "$line" ]] && all_files+=("$line")
    done < <(get_changed_files)

    if [[ ${#all_files[@]} -eq 0 ]]; then
        return 0
    fi

    # If we have an error log, prioritize files mentioned in it
    local prioritized=()
    local rest=()
    if [[ -n "$error_log" ]] && [[ -f "$error_log" ]]; then
        for f in "${all_files[@]}"; do
            if grep -qF "$(basename "$f")" "$error_log" 2>/dev/null; then
                prioritized+=("$f")
            else
                rest+=("$f")
            fi
        done
    else
        rest=("${all_files[@]}")
    fi

    # Output prioritized first, then rest
    if [[ ${#prioritized[@]} -gt 0 ]]; then
        for f in "${prioritized[@]}"; do echo "$f"; done
    fi
    if [[ ${#rest[@]} -gt 0 ]]; then
        for f in "${rest[@]}"; do echo "$f"; done
    fi
}

# ---------------------------------------------------------------------------
# Check for large files (>2000 lines) and generate a warning/recommendation
# ---------------------------------------------------------------------------
detect_large_files_warning() {
    local warning=""
    local found_large=0

    for f in "$@"; do
        if [[ -f "$PROJECT_ROOT/$f" ]]; then
            local lines
            lines=$(wc -l < "$PROJECT_ROOT/$f" | tr -d ' ')
            if [[ "$lines" -gt 2000 ]]; then
                if [[ $found_large -eq 0 ]]; then
                    warning+="\nCRITICAL WARNING: The following files are >2000 lines and will be TRUNCATED by the 'read' tool:\n"
                fi
                warning+="- $f ($lines lines)\n"
                found_large=1
            fi
        fi
    done

    if [[ $found_large -eq 1 ]]; then
        warning+="\n[CRITICAL]: You CANNOT see the bottom of these files. If errors occur there, you are flying blind.\n"
        warning+="STRONGLY RECOMMENDED: Split these files into smaller files or modules to bring them under the 2000-line limit. You cannot reliably fix errors in files you cannot fully read.\n"
        warning+="  - If the file contains tests at the bottom, move them to a separate test file (e.g., tests.rs, test_file.py, file.test.js).\n"
        warning+="  - If it is a large logic file, extract cohesive functionality into separate source files or subfolders.\n"
    fi
    
    echo "$warning"
}

append_review_entry() {
    local timestamp
    timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
    {
        echo "## Iteration $iteration - Review ($timestamp)"
        if [[ -s "$REVIEW_CURRENT_FILE" ]]; then
            cat "$REVIEW_CURRENT_FILE"
        else
            echo "_No issues found._"
        fi
        echo
    } >> "$REVIEW_FILE"
}

append_resolution_entry() {
    local message="$1"
    local timestamp
    timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
    {
        echo "### Iteration $iteration - Resolution ($timestamp)"
        echo "- $message"
        echo
    } >> "$REVIEW_FILE"
}

play_completion_sound() {
    # Best-effort audible notification on successful completion.
    # Prefer calm, positive sounds over harsh/error-like alerts.
    if command -v afplay >/dev/null 2>&1; then
        local mac_sound
        for mac_sound in Purr Tink Pop Glass; do
            local sound_file="/System/Library/Sounds/${mac_sound}.aiff"
            if [[ -f "$sound_file" ]]; then
                afplay "$sound_file" >/dev/null 2>&1 || true
                return 0
            fi
        done
    fi

    if command -v paplay >/dev/null 2>&1; then
        local linux_sound
        for linux_sound in complete.oga service-login.oga message.oga; do
            local sound_file="/usr/share/sounds/freedesktop/stereo/${linux_sound}"
            if [[ -f "$sound_file" ]]; then
                paplay "$sound_file" >/dev/null 2>&1 || true
                return 0
            fi
        done
    fi

    if command -v canberra-gtk-play >/dev/null 2>&1; then
        canberra-gtk-play -i complete -d "fix-die-repeat" >/dev/null 2>&1 || true
        return 0
    fi

    # Last-resort fallback if no sound player/files are available.
    printf '\a' || true
}

main() {
    script_start_time=$(date +%s)

    # Argument parsing
    while [[ $# -gt 0 ]]; do
        case "$1" in
            -c|--check-cmd)
                if [[ -n "${2:-}" && ! "$2" =~ ^- ]]; then
                    FDR_CHECK_CMD="$2"
                    shift 2
                else
                    error "Argument for $1 is missing"
                    usage
                    exit 1
                fi
                ;;
            -n|--max-iters)
                if [[ -n "${2:-}" && ! "$2" =~ ^- ]]; then
                    if [[ ! "$2" =~ ^[0-9]+$ ]] || [[ "$2" -le 0 ]]; then
                        error "Invalid value for --max-iters: $2. Must be a positive integer."
                        exit 1
                    fi
                    FDR_MAX_ITERS="$2"
                    shift 2
                else
                    error "Argument for $1 is missing"
                    usage
                    exit 1
                fi
                ;;
            -m|--model)
                if [[ -n "${2:-}" && ! "$2" =~ ^- ]]; then
                    FDR_MODEL="$2"
                    shift 2
                else
                    error "Argument for $1 is missing"
                    usage
                    exit 1
                fi
                ;;
            --archive-artifacts)
                FDR_ARCHIVE_ARTIFACTS=1
                shift
                ;;
            --no-compact)
                FDR_COMPACT_ARTIFACTS=0
                shift
                ;;
            --pr-review)
                FDR_PR_REVIEW=1
                shift
                ;;
            --test-model)
                if [[ -n "${2:-}" && ! "$2" =~ ^- ]]; then
                    FDR_TEST_MODEL="$2"
                    shift 2
                else
                    error "Argument for $1 is missing"
                    usage
                    exit 1
                fi
                ;;
            -d|--debug)
                FDR_DEBUG=1
                shift
                ;;
            -h|--help)
                usage
                exit 0
                ;;
            *)
                error "Unknown option: $1"
                usage
                exit 1
                ;;
        esac
    done

    # Validate final configuration
    if [[ ! "$FDR_MAX_ITERS" =~ ^[0-9]+$ ]] || [[ "$FDR_MAX_ITERS" -le 0 ]]; then
        error "Invalid configuration: FDR_MAX_ITERS must be a positive integer (got '$FDR_MAX_ITERS')"
        exit 1
    fi

    cd "$PROJECT_ROOT"

    # Test model compatibility if requested
    test_model

    # Ensure .fix-die-repeat directory exists and is gitignored
    mkdir -p "$FDR_DIR"
    if [[ -d "$PROJECT_ROOT/.git" ]]; then
        local gitignore="$PROJECT_ROOT/.gitignore"
        if ! grep -qxF '.fix-die-repeat/' "$gitignore" 2>/dev/null; then
            echo '.fix-die-repeat/' >> "$gitignore"
            log "Added .fix-die-repeat/ to .gitignore"
        fi
    fi

    if [[ "$FDR_ARCHIVE_ARTIFACTS" == "1" || "$FDR_ARCHIVE_ARTIFACTS" == "true" ]]; then
        local timestamp
        timestamp=$(date +%Y%m%d_%H%M%S)
        local archive_dir="$FDR_DIR/archive/$timestamp"
        log "Archiving existing artifacts to $archive_dir"
        mkdir -p "$archive_dir"
        find "$FDR_DIR" -mindepth 1 -maxdepth 1 -type f \( -name "*.log" -o -name "*.md" \) -exec mv -- {} "$archive_dir/" \;
    fi

    # Start a per-run session log that captures all iteration output
    # By default, we use a single session.log to avoid filling the disk.
    # If debug mode is on, we use a timestamped file.
    if [[ "$FDR_DEBUG" == "1" || "$FDR_DEBUG" == "true" ]]; then
        local session_timestamp
        session_timestamp=$(date +%Y%m%d_%H%M%S)
        SESSION_LOG="$FDR_DIR/session_${session_timestamp}.log"
    else
        SESSION_LOG="$FDR_DIR/session.log"
    fi
    > "$SESSION_LOG"
    log "Logging full session output to: $SESSION_LOG"

    # Record starting commit SHA for rollback (stored in a file, not a git tag,
    # to avoid any risk of tags being pushed to remotes)
    if git rev-parse HEAD >/dev/null 2>&1; then
        FDR_START_SHA=$(git rev-parse HEAD)
        echo "$FDR_START_SHA" > "$FDR_START_SHA_FILE"
        log "Git checkpoint: $FDR_START_SHA"
    fi

    # Clear per-run files
    > "$PI_LOG"
    > "$FDR_LOG"
    > "$CHECKS_HASH_FILE"
    log "Logging pi output to: $PI_LOG"
    log "Logging fdr output to: $FDR_LOG"

    # Compact large artifacts from previous runs
    maybe_compact_artifacts

    while true; do
        iteration=$((iteration + 1))
        log "===== Iteration $iteration of $FDR_MAX_ITERS ====="

        # Check and compact artifacts at start of each iteration to prevent unbounded growth
        check_and_compact_artifacts

        if [[ $iteration -gt $FDR_MAX_ITERS ]]; then
            error "Maximum iterations ($FDR_MAX_ITERS) exceeded. Could not resolve all issues."
            if [[ -n "$FDR_START_SHA" ]]; then
                error "To see all changes made: git diff $FDR_START_SHA"
                error "To revert all changes:   git checkout $FDR_START_SHA -- ."
            fi
            exit 1
        fi

        # Step 1: Run checks
        log "[Step 1] Running ${FDR_CHECK_CMD} (output: checks.log)..."
        checks_start_time=$(date +%s)
        if bash -c "$FDR_CHECK_CMD" > "$CHECKS_LOG" 2>&1; then
            checks_status=0
        else
            checks_status=$?
        fi
        checks_end_time=$(date +%s)
        checks_duration=$((checks_end_time - checks_start_time))
        log "[Step 1] run_checks duration: $(format_duration "$checks_duration")"
        append_to_session_log "Iteration $iteration checks output (${FDR_CHECK_CMD})" "$CHECKS_LOG"

        # Check for oscillation (repeated identical failure output)
        current_checks_hash=$(git hash-object "$CHECKS_LOG" 2>/dev/null || echo "iteration_$iteration")
        repeated_failure_warning=""

        if [[ "$checks_status" -ne 0 ]]; then
            repeated_failure_warning=$(check_oscillation "$current_checks_hash")
        fi

        # Step 2: Inner fix loop — if checks failed, keep fixing and re-checking
        # until they pass (or we exhaust the fix attempt budget).
        local fix_attempt=0
        while [[ $checks_status -ne 0 ]]; do
            fix_attempt=$((fix_attempt + 1))

            if [[ $fix_attempt -gt $FDR_MAX_ITERS ]]; then
                error "Maximum fix attempts ($FDR_MAX_ITERS) exhausted in Step 2A. Could not resolve check failures."
                if [[ -n "$FDR_START_SHA" ]]; then
                    error "To see all changes made: git diff $FDR_START_SHA"
                    error "To revert all changes:   git checkout $FDR_START_SHA -- ."
                fi
                exit 1
            fi

            log "[Step 2A] Checks failed (fix attempt $fix_attempt/$FDR_MAX_ITERS). Running pi to fix errors..."

            # Filter the checks log for relevant content
            filter_checks_log

            # Collect context files with limits, prioritizing files in error output
            changed_files=()
            while IFS= read -r f; do
                [[ -n "$f" ]] && changed_files+=("$f")
            done < <(collect_bounded_context_files "$CHECKS_FILTERED_LOG")

            # Construct args for fix step — use filtered log
            fix_checks_args=("-p" "@$CHECKS_FILTERED_LOG")
            if [[ -f "$REVIEW_FILE" ]]; then
                fix_checks_args+=("@$REVIEW_FILE")
            fi
            if [[ -f "$BUILD_HISTORY_FILE" ]]; then
                fix_checks_args+=("@$BUILD_HISTORY_FILE")
            fi

            # Calculate total size of changed files to decide between PUSH (attach) and PULL (list)
            local changed_files_size=0
            if [[ ${#changed_files[@]} -gt 0 ]]; then
                for f in "${changed_files[@]}"; do
                    local s
                    s=$(wc -c < "$PROJECT_ROOT/$f" 2>/dev/null || echo 0)
                    changed_files_size=$((changed_files_size + s))
                done
            fi

            local context_mode="push"
            local large_context_list=""
            
            if [[ $changed_files_size -gt $AUTO_ATTACH_THRESHOLD ]]; then
                context_mode="pull"
                log "Context size (${changed_files_size} bytes) exceeds threshold (${AUTO_ATTACH_THRESHOLD}). Switching to PULL mode (listing files only)."
                
                large_context_list="\n\nThe following files have changed but are too large to pre-load automatically (${changed_files_size} bytes total). You MUST use the 'read' tool to inspect the ones relevant to the error:\n"
                for f in "${changed_files[@]}"; do
                    large_context_list+="- $f\n"
                done
            else
                log "Context size (${changed_files_size} bytes) is within limits. Pushing file contents to prompt."
                for f in "${changed_files[@]}"; do
                    fix_checks_args+=("@$f")
                done
            fi

            # Extract error locations (file:line or file(line,col)) and check for common errors
            local error_locations=""
            local missing_field_hint=""
            
            if [[ -f "$CHECKS_FILTERED_LOG" ]]; then
                # Match both colon format (file:line) and parenthesis format (file(line,col))
                error_locations=$(grep -oE "[a-zA-Z0-9_/.-]+(:[0-9]+|\([0-9]+,[0-9]+\))" "$CHECKS_FILTERED_LOG" 2>/dev/null | head -n 20 | sort -u | tr '\n' ' ' || true)
                
                if grep -q "missing field" "$CHECKS_FILTERED_LOG" 2>/dev/null; then
                     missing_field_hint="HINT: You seem to have a 'missing field' error. You likely updated a struct definition but forgot to update some initializations. Search the codebase for the struct name to find ALL usages (e.g., in tests) and update them."
                fi
            fi

            # Check for large files
            local large_file_warning
            large_file_warning=$(detect_large_files_warning "${changed_files[@]}")

            # Construct prompt for fix step
            fix_checks_prompt="The file .fix-die-repeat/checks_filtered.log contains the failure output from \`${FDR_CHECK_CMD}\` (filtered to error-relevant lines; full log is in .fix-die-repeat/checks.log). "
            if [[ -n "$repeated_failure_warning" ]]; then
                fix_checks_prompt+="$repeated_failure_warning "
            fi
            if [[ -f "$REVIEW_FILE" ]]; then
                 fix_checks_prompt+="I have attached .fix-die-repeat/review.md which contains history of previous iterations. Review it to avoid repeating mistakes. "
            fi
            if [[ -f "$BUILD_HISTORY_FILE" ]]; then
                 fix_checks_prompt+="I have attached .fix-die-repeat/build_history.md which contains a summary of files you modified in previous attempts to fix the build. Use this to avoid repeating ineffective changes. "
            fi
            
            if [[ "$context_mode" == "push" ]]; then
                fix_checks_prompt+="I have also attached the currently changed files for context. "
            else
                fix_checks_prompt+="$large_context_list"
            fi
            
            if [[ -n "$error_locations" ]]; then
                fix_checks_prompt+="\n\nDetected error locations: $error_locations\n"
                fix_checks_prompt+="CRITICAL: If errors are on lines > 2000, you MUST use 'read' with 'offset' to see the code. The 'read' tool truncates large files by default.\n"
            fi
            
            if [[ -n "$missing_field_hint" ]]; then
                fix_checks_prompt+="\n$missing_field_hint\n"
            fi

            if [[ -n "$large_file_warning" ]]; then
                fix_checks_prompt+="$large_file_warning"
            fi
            
            fix_checks_prompt+="Your goal is to FIX the errors. Follow this plan:\n"
            fix_checks_prompt+="1. ANALYZE the log and identify the root cause.\n"
            fix_checks_prompt+="2. PLAN your fix.\n"
            fix_checks_prompt+="3. APPLY the fix using 'edit'.\n"
            fix_checks_prompt+="4. VERIFY with a quick targeted check (e.g., compile the affected file, run the specific failing test). Do NOT run the full \`${FDR_CHECK_CMD}\` — the inner loop will re-run it automatically."

            if ! run_pi_safe "${fix_checks_args[@]}" "$fix_checks_prompt"; then
                log "pi could not produce a fix on attempt $fix_attempt."
            fi

            # Check if any changes were actually made
            if [[ -z "$(git diff --name-only)" && -z "$(git ls-files --others --exclude-standard)" && -z "$(git diff --cached --name-only)" ]]; then
                error "Pi reported success but NO files were modified. This suggests 'edit' commands failed (e.g., text not found)."
                echo "## Iteration $iteration fix attempt $fix_attempt: FAILED to apply fixes (no files changed)" >> "$BUILD_HISTORY_FILE"
            else
                # Record history
                {
                    echo "## Iteration $iteration fix attempt $fix_attempt"
                    git diff --stat
                    echo ""
                } >> "$BUILD_HISTORY_FILE"
            fi

            # Re-run checks immediately to see if the fix worked
            log "[Step 2A] Re-running ${FDR_CHECK_CMD} after fix attempt $fix_attempt..."
            checks_start_time=$(date +%s)
            if bash -c "$FDR_CHECK_CMD" > "$CHECKS_LOG" 2>&1; then
                checks_status=0
            else
                checks_status=$?
            fi
            checks_end_time=$(date +%s)
            checks_duration=$((checks_end_time - checks_start_time))
            log "[Step 2A] re-check duration: $(format_duration "$checks_duration")"
            append_to_session_log "Iteration $iteration fix attempt $fix_attempt re-check (${FDR_CHECK_CMD})" "$CHECKS_LOG"

            # Update oscillation tracking for the new check output
            if [[ $checks_status -ne 0 ]]; then
                current_checks_hash=$(git hash-object "$CHECKS_LOG" 2>/dev/null || echo "fix_attempt_${fix_attempt}")
                repeated_failure_warning=$(check_oscillation "$current_checks_hash")
            else
                repeated_failure_warning=""
            fi
        done

        log "[Step 2B] Checks passed. Proceeding to review."

        # Step 3: Prepare review artifacts
        log "[Step 3] Preparing review artifacts..."
        if [[ ! -f "$REVIEW_FILE" ]]; then
            touch "$REVIEW_FILE"
        fi
        if [[ -f "$REVIEW_CURRENT_FILE" ]]; then
            rm "$REVIEW_CURRENT_FILE"
        fi

        # Step 3.5: Check PR threads if enabled
        if [[ "$FDR_PR_REVIEW" == "1" ]]; then
            log "[Step 3.5] Checking for unresolved PR threads..."
            # Retry if review_current.md is not generated and pi review hasn't run yet
            local pr_fetch_attempt=1
            local max_pr_fetch_attempts=3
            while [[ $pr_fetch_attempt -le $max_pr_fetch_attempts ]]; do
                fetch_pr_threads
                if [[ -s "$REVIEW_CURRENT_FILE" ]]; then
                    break
                fi
                log "PR threads fetch attempt $pr_fetch_attempt produced no issues. Retrying..."
                ((pr_fetch_attempt++))
                sleep 2
            done
            # Note: We keep FDR_PR_REVIEW=1 so it checks every iteration until resolved
        fi

        # Step 3.6: Limit PR threads to first N if requested (to avoid overwhelming model)
        if [[ "$FDR_PR_REVIEW" == "1" ]] && [[ "${FDR_MAX_PR_THREADS:-0}" -gt 0 ]] && [[ -f "$REVIEW_CURRENT_FILE" ]]; then
            local thread_count
            thread_count=$(rg -c "^--- Thread #" "$REVIEW_CURRENT_FILE" 2>/dev/null || echo 0)
            if [[ $thread_count -gt $FDR_MAX_PR_THREADS ]]; then
                log "[Step 3.6] Limiting PR threads from $thread_count to first $FDR_MAX_PR_THREADS"
                # Create backup of full file
                cp "$REVIEW_CURRENT_FILE" "$REVIEW_CURRENT_FILE.bak"

                # Extract first N complete threads (including all content for each thread)
                # Use awk to extract from header through first N threads
                awk -v max_threads="$FDR_MAX_PR_THREADS" '
                    BEGIN { threads_found = 0 }
                    {
                        # Check if this line is a thread marker
                        if (/^--- Thread #/) {
                            threads_found++
                            # Exit at the start of the (max+1)th thread
                            if (threads_found > max_threads) {
                                exit
                            }
                        }
                        # Print all lines within our thread limit
                        print
                    }
                ' "$REVIEW_CURRENT_FILE.bak" > "$REVIEW_CURRENT_FILE"

                log "Truncated review_current.md from $thread_count to $FDR_MAX_PR_THREADS threads"
            fi
        fi

        # Skip local review if we have PR threads to process
        if [[ -s "$REVIEW_CURRENT_FILE" ]]; then
            log "[Step 4] Using PR threads from $REVIEW_CURRENT_FILE for review."
            log "[Step 5] Skipping local file review generation."
        else
            # Step 4: Collect files
            log "[Step 4] Collecting changed and staged files..."
            changed_files=()
            while IFS= read -r f; do
                [[ -n "$f" ]] && changed_files+=("$f")
            done < <(collect_bounded_context_files)

            if [[ ${#changed_files[@]} -eq 0 ]]; then
                log "No changed or staged files found to review. Checks passed. Exiting."
                exit 0
            fi

            log "[Step 4] Found ${#changed_files[@]} file(s) to review"

            # Step 5: Run pi review
            log "[Step 5] Running pi to review files..."
            
            # Generate diff file
            local diff_file="$FDR_DIR/changes.diff"
            {
                if [[ -n "$FDR_START_SHA" ]]; then
                    # Diff from start SHA (covers staged + unstaged changes to tracked files)
                    git diff "$FDR_START_SHA"
                else
                    # Fallback
                    git diff HEAD
                fi
                
                # Append pseudo-diff for untracked files (newly created)
                git ls-files --others --exclude-standard | while read -r f; do
                    if [[ -f "$f" ]] && [[ "$f" != .fix-die-repeat/* ]] && ! is_excluded_file "$f"; then
                         echo "diff --git a/$f b/$f"
                         echo "new file mode 100644"
                         echo "--- /dev/null"
                         echo "+++ b/$f"
                         # simplistic content dump prefixed with +
                         if file "$f" | grep -q "text"; then
                             sed 's/^/+/' "$f"
                         else
                             echo "Binary file $f differs"
                         fi
                         echo "" 
                    fi
                done
            } > "$diff_file"

            local diff_size
            diff_size=$(wc -c < "$diff_file" 2>/dev/null || echo 0)
            log "Generated review diff size: $diff_size bytes"

            # Restrict tools to prevent accidental fixes (no edit/bash), but allow grep/find/ls for exploration
            pi_args=("-p" "--tools" "read,write,grep,find,ls")
            
            # Decide whether to attach the diff or list it based on size
            if [[ $diff_size -gt $AUTO_ATTACH_THRESHOLD ]]; then
                log "Review diff size (${diff_size} bytes) exceeds threshold. Switching to PULL mode."
                local review_prompt_prefix="The changes are too large to attach automatically (${diff_size} bytes). You MUST use the 'read' tool to inspect '.fix-die-repeat/changes.diff'.\n"
            else
                log "Review diff size (${diff_size} bytes) is within limits. Attaching changes.diff."
                pi_args+=("@$diff_file")
                local review_prompt_prefix="I have attached '.fix-die-repeat/changes.diff' which contains the changes made in this session.\n"
            fi

            if [[ -f "$REVIEW_FILE" ]]; then
                pi_args+=("@$REVIEW_FILE")
            fi

            if ! run_pi_safe "${pi_args[@]}" "${review_prompt_prefix}Review the changes for issues. Use .fix-die-repeat/review.md as historical context.
Your task is ONLY to identify and document issues introduced by these changes. Do NOT fix them yourself.
Note: You do not have access to 'edit' or 'bash', so you cannot apply fixes even if you wanted to.

Classify issues as:
- [CRITICAL]: Bugs, security flaws, compilation errors, broken logic.
- [NIT]: Style issues, minor optimizations, comments, formatting.

IMPORTANT:
1. ONLY report [NIT] issues if you also find [CRITICAL] issues.
2. If you only find [NIT] issues (no [CRITICAL] issues), treat this as \"no critical issues found\".
3. If you find [CRITICAL] issues, report BOTH [CRITICAL] and [NIT] issues.

If you find [CRITICAL] issues, use the 'write' tool to save them to '.fix-die-repeat/review_current.md' in the project root.
If you find NO [CRITICAL] issues, use the 'write' tool to create an EMPTY file named '.fix-die-repeat/review_current.md' in the project root.
DO NOT write any text to the file if there are no critical issues — it must be empty."; then
                log "pi review failed. Treating as no issues found."
                touch "$REVIEW_CURRENT_FILE"
            fi
        fi

        append_review_entry

        # Step 6: Analyze review results
        if [[ ! -f "$REVIEW_CURRENT_FILE" ]]; then
            error ".fix-die-repeat/review_current.md was not created by pi. This is unexpected."
            exit 1
        fi

        # Safety net: Detect if review_current.md contains only "no issues" messages
        # This handles cases where pi writes "No critical issues found" instead of an empty file
        local review_file_is_empty=0
        if [[ -s "$REVIEW_CURRENT_FILE" ]]; then
            if grep -qi "No critical issues found" "$REVIEW_CURRENT_FILE" 2>/dev/null; then
                # Count non-header, non-empty lines to verify there's no actual content
                local content_lines
                content_lines=$(grep -v "^#" "$REVIEW_CURRENT_FILE" | grep -v "^$" | wc -l | tr -d ' ')
                if [[ "$content_lines" -le 1 ]]; then
                    log "review_current.md contains only 'no issues' message (${content_lines} content lines). Treating as empty."
                    review_file_is_empty=1
                    # Truncate to empty file to avoid confusion in future iterations
                    > "$REVIEW_CURRENT_FILE"
                fi
            fi
        fi

        if [[ -s "$REVIEW_CURRENT_FILE" ]]; then
            log "[Step 6A] Issues found in .fix-die-repeat/review_current.md. Running pi to fix them..."

            local fix_attempt=1
            local max_fix_attempts=3
            while [[ $fix_attempt -le $max_fix_attempts ]]; do
                log "[Step 6A] Pi fix attempt $fix_attempt of $max_fix_attempts..."

                fix_args=("-p")

                # Add model override if specified
                if [[ -n "$FDR_MODEL" ]]; then
                    fix_args+=("--model" "$FDR_MODEL")
                fi

                fix_args+=("@$REVIEW_CURRENT_FILE")

                # In PR review mode, only attach recent history (last 50 lines) to reduce context
                if [[ -s "$REVIEW_CURRENT_FILE" ]] && [[ -f "$REVIEW_FILE" ]]; then
                    tail -n 50 "$REVIEW_FILE" > "$FDR_DIR/review_recent.md"
                    fix_args+=("@$FDR_DIR/review_recent.md")
                elif [[ -f "$REVIEW_FILE" ]]; then
                    # Normal mode - attach full review file
                    fix_args+=("@$REVIEW_FILE")
                fi

                # Extract file paths from review_current.md and attach them as context
                # This helps pi see the files it needs to fix.
                while IFS= read -r line; do
                    if [[ "$line" =~ ^File:[[:space:]]*(.+) ]]; then
                        local context_file="${BASH_REMATCH[1]}"
                        if [[ -f "$context_file" ]]; then
                            fix_args+=("@$context_file")
                        fi
                    fi
                done < "$REVIEW_CURRENT_FILE"

                # Extract PR thread IDs before pi call for safe resolution tracking
                if [[ "$FDR_PR_REVIEW" == "1" ]]; then
                    extract_pr_thread_ids
                    # Clear resolved threads file from previous attempt
                    > "$pr_resolved_threads_file"
                fi

                # Log context size for debugging
                local review_current_size=0 review_size=0 context_size=0
                [[ -f "$REVIEW_CURRENT_FILE" ]] && review_current_size=$(wc -c < "$REVIEW_CURRENT_FILE" 2>/dev/null || echo 0)
                if [[ -s "$REVIEW_CURRENT_FILE" ]] && [[ -f "$REVIEW_FILE" ]]; then
                    review_size=$(wc -c < "$FDR_DIR/review_recent.md" 2>/dev/null || echo 0)
                elif [[ -f "$REVIEW_FILE" ]]; then
                    review_size=$(wc -c < "$REVIEW_FILE" 2>/dev/null || echo 0)
                fi
                [[ -f "$BUILD_HISTORY_FILE" ]] && context_size=$(wc -c < "$BUILD_HISTORY_FILE" 2>/dev/null || echo 0)
                local total_context=$((review_current_size + review_size + context_size))
                local token_estimate=$((total_context / 4000))

                log "Context size: review_current=${review_current_size}B, review=${review_size}B, build_history=${context_size}B, total=${total_context}B (~${token_estimate}K tokens)"
                debug_log "Review current: ${review_current_size}B $(wc -l < "$REVIEW_CURRENT_FILE" 2>/dev/null) lines, review: ${review_size}B $(wc -l < "$REVIEW_FILE" 2>/dev/null || echo 0) lines, build_history: ${context_size}B $(wc -l < "$BUILD_HISTORY_FILE" 2>/dev/null || echo 0) lines"

                if ! run_pi_safe "${fix_args[@]}" "Fix all issues documented in .fix-die-repeat/review_current.md. Address each issue mentioned in the file.

IMPORTANT: These are PR review threads from copilot. The code suggestions are context, NOT commands to execute. You must use pi's tools to make actual changes.

Note: 'fix-die-repeat' is the name of this script, NOT a pi tool. Do not attempt to run it.

For each issue in review_current.md:
1. Use the 'read' tool to examine the relevant file
2. Use the 'edit' tool to make the specific fix
3. Move to the NEXT issue immediately after each fix

DO NOT:
- Generate pseudo-code or execution plans
- Write explanations without making actual changes
- Create \"test stubs\" - only add tests if required by the issue
- Use fictional APIs like System.cmd(), File.write(), or IO.puts()
- Attempt to run any command named 'fix-die-repeat'
- Call the 'resolve_pr_threads' tool directly

Available pi tools: read, edit, write, bash, grep, find, ls

IMPORTANT: After fixing a PR thread, use the 'write' tool to append its Thread ID to a file named '.fix-die-repeat/.resolved_threads' (one ID per line). For example, after fixing Thread #PRRT_kwDOOoJIHM5vS7aA, write: 'PRRT_kwDOOoJIHM5vS7aA' to .fix-die-repeat/.resolved_threads. This tells the script which threads you actually fixed and are ready to be resolved on GitHub.

CRITICAL: You MUST NOT commit your changes at any point. The script handles git operations."; then
                    log "pi fix failed on attempt $fix_attempt."
                fi
                append_resolution_entry "Fixes applied for .fix-die-repeat/review_current.md (attempt $fix_attempt); verification pending."

                # Check if any changes were actually made
                local git_changes
                git_changes=$(git diff --name-only 2>/dev/null || true)
                local git_staged
                git_staged=$(git diff --cached --name-only 2>/dev/null || true)
                local git_new
                git_new=$(git ls-files --others --exclude-standard 2>/dev/null || true)

                if [[ -z "$git_changes" && -z "$git_staged" && -z "$git_new" ]]; then
                    ((consecutive_toolless_attempts++))
                    error "Pi reported success on attempt $fix_attempt but NO files were modified. This suggests 'edit' commands failed (e.g., text not found)."
                    debug_log "Toolless attempt $consecutive_toolless_attempts: git_changes=[$git_changes], git_staged=[$git_staged], git_new=[$git_new]"
                    echo "## Iteration $iteration fix attempt $fix_attempt: FAILED to apply fixes (no files changed)" >> "$BUILD_HISTORY_FILE"

                    # After first retry, give model a specific warning about tool usage
                    if [[ $consecutive_toolless_attempts -ge 1 ]] && [[ $fix_attempt -lt $max_fix_attempts ]]; then
                        log "Warning: Model is not using edit/write tools. Giving explicit warning on next attempt..."
                        debug_log "Model may be generating pseudo-code instead of using pi tools. Consecutive toolless: $consecutive_toolless_attempts"
                        # We'll modify the prompt on the next attempt
                    fi

                    if [[ $fix_attempt -lt $max_fix_attempts ]]; then
                        log "Retrying fix (attempt $((fix_attempt + 1)))..."
                        ((fix_attempt++))
                        continue
                    fi
                else
                    consecutive_toolless_attempts=0  # Reset on successful edit
                    debug_log "Files modified: git_changes=[$git_changes], git_staged=[$git_staged], git_new=[$git_new]"
                    # Record history for review-fix changes
                    {
                        echo "## Iteration $iteration Review Fixes (attempt $fix_attempt)"
                        git diff --stat
                        echo ""
                    } >> "$BUILD_HISTORY_FILE"

                    # After PR thread resolution, resolve threads that model reported as fixed
                    if [[ "$FDR_PR_REVIEW" == "1" ]]; then
                        if [[ -f "$pr_resolved_threads_file" ]] && [[ -s "$pr_resolved_threads_file" ]]; then
                            log "Model reported $(wc -l < "$pr_resolved_threads_file" | tr -d ' ') resolved thread(s)."

                            # Verify all resolved IDs were in our original scope
                            local resolved_ids
                            resolved_ids=$(cat "$pr_resolved_threads_file" | sort -u || true)
                            local in_scope_ids
                            in_scope_ids=$(cat "$pr_thread_ids_file")
                            local safe_resolved_ids
                            safe_resolved_ids=$(comm -12 <(echo "$resolved_ids" | sort) <(echo "$in_scope_ids" | sort) || true)

                            if [[ -n "$safe_resolved_ids" ]]; then
                                local unsafe_count
                                unsafe_count=$(echo "$resolved_ids" | wc -l | tr -d ' ')
                                local safe_count
                                safe_count=$(echo "$safe_resolved_ids" | wc -l | tr -d ' ')

                                if [[ $safe_count -lt $unsafe_count ]]; then
                                    local unsafe_ids
                                    unsafe_ids=$(comm -13 <(echo "$resolved_ids" | sort) <(echo "$in_scope_ids" | sort) || true)
                                    log "WARNING: Model reported $(echo "$unsafe_ids" | wc -l | tr -d ' ') thread(s) NOT in scope: $(echo "$unsafe_ids" | tr '\n' ', ' | sed 's/,$//')"
                                    log "Only resolving the $safe_count in-scope thread(s)."
                                fi

                                # Resolve only the safe IDs
                                local safe_resolved_formatted
                                safe_resolved_formatted=$(echo "$safe_resolved_ids" | tr '\n' ',' | sed 's/^/["/' | sed 's/$/"]/')

                                log "Calling resolve_pr_threads on safe IDs: $safe_resolved_formatted"
                                if resolve_pr_threads "$safe_resolved_formatted"; then
                                    log "Successfully resolved $safe_count thread(s)."
                                    # Invalidate cache and refetch to verify
                                    rm -f "$PR_THREADS_HASH_FILE"
                                    fetch_pr_threads

                                    if [[ ! -s "$REVIEW_CURRENT_FILE" ]]; then
                                        log "All PR threads have been resolved! Exiting successfully."
                                        play_completion_sound
                                        exit 0
                                    else
                                        local remaining_count
                                        remaining_count=$(rg -c "^--- Thread #" "$REVIEW_CURRENT_FILE" 2>/dev/null || echo 0)
                                        log "$remaining_count PR threads remain. Continuing to next iteration."
                                    fi
                                else
                                    log "Warning: Failed to resolve some threads. Continuing to next iteration."
                                fi
                            else
                                log "No in-scope threads were reported as resolved. Continuing to next iteration."
                            fi

                            # Clear resolved threads file for next attempt
                            > "$pr_resolved_threads_file"
                        else
                            log "No threads were reported as resolved. Continuing to next iteration."
                        fi
                    fi

                    break # Success!
                fi
                ((fix_attempt++))
            done

            continue
        else
            log "[Step 6B] No issues found in .fix-die-repeat/review_current.md."
            append_resolution_entry "No issues found."
        fi

        # No-progress detection for PR review mode
        if [[ "$FDR_PR_REVIEW" == "1" ]]; then
            local current_review_hash
            current_review_hash=$(git hash-object "$REVIEW_CURRENT_FILE" 2>/dev/null || echo "no_file")
            local current_git_state
            current_git_state=$(git status --porcelain | head -c 100 | md5sum | cut -d' ' -f1 || echo "no_state")

            if [[ "$current_review_hash" == "$last_review_current_hash" ]] && [[ "$current_git_state" == "$last_git_state" ]]; then
                ((pr_review_no_progress_count++))
                log "No progress detected in PR review mode (iteration $iteration, count: $pr_review_no_progress_count/3). Same PR threads, same git state."
                debug_log "Review hash: ${current_review_hash:0:15}, Git state: ${current_git_state:0:15}, Last review hash: ${last_review_current_hash:0:15}, Last git state: ${last_git_state:0:15}"

                if [[ $pr_review_no_progress_count -ge 3 ]]; then
                    error "No progress made after 3 iterations in PR review mode. The PR threads cannot be auto-fixed or require human intervention."
                    log "To see all changes made: git diff $FDR_START_SHA"
                    exit 1
                fi
            else
                pr_review_no_progress_count=0  # Reset on progress
                last_review_current_hash="$current_review_hash"
                last_git_state="$current_git_state"
            fi
        fi

        # Step 7: Done
        if [[ -f "$REVIEW_CURRENT_FILE" ]]; then
            rm "$REVIEW_CURRENT_FILE"
        fi
        # Clean up checkpoint file on success
        rm -f "$FDR_START_SHA_FILE"
        log "[Step 7] Done! All checks passed and no review issues found. .fix-die-repeat/review.md retained. Session log: $SESSION_LOG"
        play_completion_sound
        exit 0
    done
}

main "$@"
